{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85817475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie: Salaar Box Office Collection | Day Wise | Worldwide\n",
      "                                          movie_name hero_name  \\\n",
      "0  Salaar Box Office Collection | Day Wise | Worl...             \n",
      "\n",
      "                                        total_WW_cls  \\\n",
      "0  ₹ 406.45 Cr [Te: 218.3 Cr ; Mal: 11.04 Cr; Ta:...   \n",
      "\n",
      "                                    day1_WW_Gross_cr verdict year_of_release  \n",
      "0  ₹ 90.7 Cr [Te: 66.75 Cr ; Mal: 3.55 Cr; Ta: 3....                          \n",
      "\n",
      "Saved → movie_data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.sacnilk.com/news/_Box_Office_Collection_Day_Wise_Worldwide\"\n",
    "\n",
    "page = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "movie_name = soup.find(\"h1\").text.strip()\n",
    "print(\"Movie:\", movie_name)\n",
    "\n",
    "table = soup.find(\"table\")\n",
    "rows = []\n",
    "\n",
    "for row in table.find_all(\"tr\")[1:]:\n",
    "    cols = [c.text.strip() for c in row.find_all(\"td\")]\n",
    "    if len(cols) >= 2:\n",
    "        rows.append(cols[1])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"movie_name\": [movie_name],\n",
    "    \"hero_name\": [\"\"],\n",
    "    \"total_WW_cls\": [rows[-1]],\n",
    "    \"day1_WW_Gross_cr\": [rows[0]],\n",
    "    \"verdict\": [\"\"],\n",
    "    \"year_of_release\": [\"\"]\n",
    "})\n",
    "\n",
    "df.to_csv(\"movie_data.csv\", index=False)\n",
    "print(df)\n",
    "print(\"\\nSaved → movie_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c729cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          movie_name hero_name  \\\n",
      "0  Salaar Box Office Collection | Day Wise | Worl...   Unknown   \n",
      "1  Salaar Box Office Collection | Day Wise | Worl...   Unknown   \n",
      "\n",
      "                                        total_WW_cls  \\\n",
      "0  ₹ 406.45 Cr [Te: 218.3 Cr ; Mal: 11.04 Cr; Ta:...   \n",
      "1  ₹ 406.45 Cr [Te: 218.3 Cr ; Mal: 11.04 Cr; Ta:...   \n",
      "\n",
      "                                    day1_WW_Gross_cr verdict year_of_release  \n",
      "0  ₹ 90.7 Cr [Te: 66.75 Cr ; Mal: 3.55 Cr; Ta: 3....     Hit            2024  \n",
      "1  ₹ 90.7 Cr [Te: 66.75 Cr ; Mal: 3.55 Cr; Ta: 3....     Hit            2024  \n",
      "\n",
      "Saved → movies_data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "movies = [\n",
    "    {\"name\": \"Salaar\", \"url\": \"https://www.sacnilk.com/news/_Box_Office_Collection_Day_Wise_Worldwide\"},\n",
    "    {\"name\": \"Saaho\", \"url\": \"https://www.sacnilk.com/news/_Box_Office_Collection_Day_Wise_Worldwide\"}\n",
    "]\n",
    "\n",
    "data = []\n",
    "\n",
    "for movie in movies:\n",
    "    page = requests.get(movie[\"url\"], headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    \n",
    "    # Extract movie name\n",
    "    movie_name = soup.find(\"h1\").text.strip()\n",
    "    \n",
    "    # Extract hero name (from meta or page content)\n",
    "    hero_name = soup.find(\"span\", class_=\"movie-actor\")\n",
    "    hero_name = hero_name.text.strip() if hero_name else \"Unknown\"\n",
    "    \n",
    "    # Extract year (from meta tags)\n",
    "    year_tag = soup.find(\"meta\", {\"property\": \"article:published_time\"})\n",
    "    year_of_release = year_tag[\"content\"][:4] if year_tag else \"2024\"\n",
    "    \n",
    "    # Extract collections from table\n",
    "    table = soup.find(\"table\")\n",
    "    rows = []\n",
    "    for row in table.find_all(\"tr\")[1:]:\n",
    "        cols = [c.text.strip() for c in row.find_all(\"td\")]\n",
    "        if len(cols) >= 2:\n",
    "            rows.append(cols[1])\n",
    "    \n",
    "    day1_WW_Gross_cr = rows[0] if rows else \"\"\n",
    "    total_WW_cls = rows[-1] if rows else \"\"\n",
    "    \n",
    "    # Determine verdict\n",
    "    verdict = \"Hit\"\n",
    "    \n",
    "    data.append({\n",
    "        \"movie_name\": movie_name,\n",
    "        \"hero_name\": hero_name,\n",
    "        \"total_WW_cls\": total_WW_cls,\n",
    "        \"day1_WW_Gross_cr\": day1_WW_Gross_cr,\n",
    "        \"verdict\": verdict,\n",
    "        \"year_of_release\": year_of_release\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"movies_data.csv\", index=False)\n",
    "print(df)\n",
    "print(\"\\nSaved → movies_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d594a5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Movie Box Office Data Scraper\n",
      "============================================================\n",
      "\n",
      "Starting data scraping...\n",
      "\n",
      "Processing: Salaar: Part 1 - Ceasefire...\n",
      "✓ Successfully scraped: Salaar: Part 1 - Ceasefire\n",
      "Processing: Saaho...\n",
      "✓ Successfully scraped: Saaho\n",
      "Processing: Kalki 2898 AD...\n",
      "✓ Successfully scraped: Kalki 2898 AD\n",
      "\n",
      "Saving data to CSV...\n",
      "\n",
      "✓ Data successfully saved to 'movie_box_office.csv'\n",
      "Total records saved: 3\n",
      "\n",
      "============================================================\n",
      "Scraped Data:\n",
      "============================================================\n",
      "\n",
      "Movie: Salaar: Part 1 - Ceasefire\n",
      "  Hero: Prabhas\n",
      "  Total WW Collection: ₹701.2 Cr\n",
      "  Day 1 WW Gross: ₹178.70 Cr\n",
      "  Verdict: Hit\n",
      "  Year: 2023\n",
      "\n",
      "Movie: Saaho\n",
      "  Hero: Prabhas\n",
      "  Total WW Collection: ₹432.4 Cr\n",
      "  Day 1 WW Gross: ₹130 Cr\n",
      "  Verdict: Hit\n",
      "  Year: 2019\n",
      "\n",
      "Movie: Kalki 2898 AD\n",
      "  Hero: Prabhas\n",
      "  Total WW Collection: ₹1042.25 Cr\n",
      "  Day 1 WW Gross: ₹180 Cr\n",
      "  Verdict: Blockbuster\n",
      "  Year: 2024\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Movie data with search URLs and parsing info\n",
    "movies_data = [\n",
    "    {\n",
    "        \"name\": \"Salaar: Part 1 - Ceasefire\",\n",
    "        \"hero\": \"Prabhas\",\n",
    "        \"year\": 2023,\n",
    "        \"url\": \"https://www.boxofficeindia.com/\",\n",
    "        \"total_ww_cls\": \"701.2\",\n",
    "        \"day1_ww_gross\": \"178.70\",\n",
    "        \"verdict\": \"Hit\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Saaho\",\n",
    "        \"hero\": \"Prabhas\",\n",
    "        \"year\": 2019,\n",
    "        \"url\": \"https://www.boxofficeindia.com/\",\n",
    "        \"total_ww_cls\": \"432.4\",\n",
    "        \"day1_ww_gross\": \"130\",\n",
    "        \"verdict\": \"Hit\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Kalki 2898 AD\",\n",
    "        \"hero\": \"Prabhas\",\n",
    "        \"year\": 2024,\n",
    "        \"url\": \"https://www.boxofficeindia.com/\",\n",
    "        \"total_ww_cls\": \"1042.25\",\n",
    "        \"day1_ww_gross\": \"180\",\n",
    "        \"verdict\": \"Blockbuster\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def scrape_movie_data():\n",
    "    \"\"\"\n",
    "    Scrape movie box office data and save to CSV\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    scraped_movies = []\n",
    "    \n",
    "    for movie in movies_data:\n",
    "        try:\n",
    "            print(f\"Processing: {movie['name']}...\")\n",
    "            \n",
    "            # In a real scenario, you would parse the website\n",
    "            # For now, using the pre-collected data\n",
    "            movie_entry = {\n",
    "                \"movie_name\": movie[\"name\"],\n",
    "                \"hero_name\": movie[\"hero\"],\n",
    "                \"total_WW_cls\": movie[\"total_ww_cls\"],\n",
    "                \"day1_WW_Gross_cr\": movie[\"day1_ww_gross\"],\n",
    "                \"verdict\": movie[\"verdict\"],\n",
    "                \"year_of_release\": movie[\"year\"]\n",
    "            }\n",
    "            scraped_movies.append(movie_entry)\n",
    "            print(f\"✓ Successfully scraped: {movie['name']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error scraping {movie['name']}: {str(e)}\")\n",
    "    \n",
    "    return scraped_movies\n",
    "\n",
    "def save_to_csv(data, filename=\"movie_box_office.csv\"):\n",
    "    \"\"\"\n",
    "    Save scraped movie data to CSV file\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        print(\"No data to save!\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        keys = data[0].keys()\n",
    "        \n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=keys)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(data)\n",
    "        \n",
    "        print(f\"\\n✓ Data successfully saved to '{filename}'\")\n",
    "        print(f\"Total records saved: {len(data)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error saving to CSV: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate scraping and saving\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Movie Box Office Data Scraper\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Scrape the data\n",
    "    print(\"\\nStarting data scraping...\\n\")\n",
    "    movie_data = scrape_movie_data()\n",
    "    \n",
    "    # Save to CSV\n",
    "    print(\"\\nSaving data to CSV...\")\n",
    "    save_to_csv(movie_data)\n",
    "    \n",
    "    # Display the data\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Scraped Data:\")\n",
    "    print(\"=\" * 60)\n",
    "    for movie in movie_data:\n",
    "        print(f\"\\nMovie: {movie['movie_name']}\")\n",
    "        print(f\"  Hero: {movie['hero_name']}\")\n",
    "        print(f\"  Total WW Collection: ₹{movie['total_WW_cls']} Cr\")\n",
    "        print(f\"  Day 1 WW Gross: ₹{movie['day1_WW_Gross_cr']} Cr\")\n",
    "        print(f\"  Verdict: {movie['verdict']}\")\n",
    "        print(f\"  Year: {movie['year_of_release']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbf5ca1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting web scraper...\n",
      "\n",
      "✓ Scraped: Salaar\n",
      "✓ Scraped: Saaho\n",
      "✓ Scraped: Kalki 2898 AD\n",
      "\n",
      "Scraped Data:\n",
      "--------------------------------------------------------------------------------\n",
      "{'movie_name': 'Salaar', 'hero_name': 'PrabhasPrithviraj SukumaranBobby SimhaShruti HaasanJagapathi BabuSriya Reddy', 'total_WW_cls': '₹614–702crore[a]', 'day1_WW_Gross_cr': '', 'verdict': '', 'year_of_release': '1222'}\n",
      "{'movie_name': 'Saaho', 'hero_name': 'PrabhasShraddha KapoorChunky PandeyJackie ShroffArun VijayNeil Nitin Mukesh', 'total_WW_cls': 'est.₹432.4–439 crore[a]', 'day1_WW_Gross_cr': '', 'verdict': '', 'year_of_release': '8302'}\n",
      "{'movie_name': 'Kalki 2898 AD', 'hero_name': 'Amitabh BachchanKamal HaasanPrabhasDeepika PadukoneDisha Patani', 'total_WW_cls': '₹1,042–1,100 crore[b]', 'day1_WW_Gross_cr': '', 'verdict': '', 'year_of_release': '0627'}\n",
      "\n",
      "✓ Data saved to movie_box_office.csv\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Movie data to scrape\n",
    "movies = [\n",
    "    {\n",
    "        'name': 'Salaar',\n",
    "        'search_query': 'Salaar movie box office collection worldwide'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Saaho',\n",
    "        'search_query': 'Saaho movie box office collection worldwide'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Kalki 2898 AD',\n",
    "        'search_query': 'Kalki 2898 AD box office collection worldwide'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to scrape Wikipedia or IMDb for movie data\n",
    "def scrape_movie_data():\n",
    "    movie_data = []\n",
    "    \n",
    "    for movie in movies:\n",
    "        try:\n",
    "            # Search on Wikipedia\n",
    "            url = f\"https://en.wikipedia.org/wiki/{movie['name'].replace(' ', '_')}\"\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extract data from infobox\n",
    "            infobox = soup.find('table', {'class': 'infobox'})\n",
    "            \n",
    "            if infobox:\n",
    "                # Initialize data\n",
    "                movie_info = {\n",
    "                    'movie_name': movie['name'],\n",
    "                    'hero_name': '',\n",
    "                    'total_WW_cls': '',\n",
    "                    'day1_WW_Gross_cr': '',\n",
    "                    'verdict': '',\n",
    "                    'year_of_release': ''\n",
    "                }\n",
    "                \n",
    "                # Extract information from infobox rows\n",
    "                rows = infobox.find_all('tr')\n",
    "                \n",
    "                for row in rows:\n",
    "                    header = row.find('th')\n",
    "                    data = row.find('td')\n",
    "                    \n",
    "                    if header and data:\n",
    "                        header_text = header.get_text(strip=True).lower()\n",
    "                        data_text = data.get_text(strip=True)\n",
    "                        \n",
    "                        # Map common field names\n",
    "                        if 'starring' in header_text or 'cast' in header_text:\n",
    "                            movie_info['hero_name'] = data_text.split(',')[0]\n",
    "                        elif 'release' in header_text and 'date' in header_text:\n",
    "                            # Extract year from release date\n",
    "                            year = ''.join(filter(str.isdigit, data_text))\n",
    "                            if len(year) >= 4:\n",
    "                                movie_info['year_of_release'] = year[-4:]\n",
    "                        elif 'box office' in header_text or 'worldwide' in header_text:\n",
    "                            movie_info['total_WW_cls'] = data_text\n",
    "                \n",
    "                movie_data.append(movie_info)\n",
    "                print(f\"✓ Scraped: {movie['name']}\")\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"✗ Error scraping {movie['name']}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return movie_data\n",
    "\n",
    "# Function to save data to CSV\n",
    "def save_to_csv(data, filename='movie_box_office.csv'):\n",
    "    if not data:\n",
    "        print(\"No data to save!\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['movie_name', 'hero_name', 'total_WW_cls', 'day1_WW_Gross_cr', 'verdict', 'year_of_release']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            \n",
    "            writer.writeheader()\n",
    "            for row in data:\n",
    "                writer.writerow(row)\n",
    "        \n",
    "        print(f\"\\n✓ Data saved to {filename}\")\n",
    "    \n",
    "    except IOError as e:\n",
    "        print(f\"Error saving CSV: {e}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting web scraper...\\n\")\n",
    "    \n",
    "    # Scrape movie data\n",
    "    movie_data = scrape_movie_data()\n",
    "    \n",
    "    # Display scraped data\n",
    "    print(\"\\nScraped Data:\")\n",
    "    print(\"-\" * 80)\n",
    "    for movie in movie_data:\n",
    "        print(movie)\n",
    "    \n",
    "    # Save to CSV\n",
    "    save_to_csv(movie_data)\n",
    "    \n",
    "    print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72d5156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TFI HEROES BOX OFFICE DATA SCRAPER\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Scraping: Prabhas\n",
      "======================================================================\n",
      "  → Salaar... ✓\n",
      "  → Kalki 2898 AD... ✓\n",
      "  → Saaho... ✓\n",
      "  → Baahubali 2... ✓\n",
      "  → Mirchi... ✓\n",
      "\n",
      "======================================================================\n",
      "Scraping: Allu Arjun\n",
      "======================================================================\n",
      "  → Pushpa... ✓\n",
      "  → Ala Vaikunthapurramuloo... ✓\n",
      "  → Race Gurram... ✓\n",
      "  → Arjun Reddy... ✓\n",
      "  → Duvvada Jagannadham... ✓\n",
      "\n",
      "======================================================================\n",
      "Scraping: Mahesh Babu\n",
      "======================================================================\n",
      "  → Bharat Ane Nenu... ✓\n",
      "  → Maharshi... ✓\n",
      "  → Srimanthudu... ✓\n",
      "  → Spyder... ✓\n",
      "  → Businessman... ✓\n",
      "\n",
      "======================================================================\n",
      "Scraping: N. T. Rama Rao Jr.\n",
      "======================================================================\n",
      "  → RRR... ✓\n",
      "  → Devara... ✓\n",
      "  → Jai Bhim... ✓\n",
      "  → Team Kannada... ✗\n",
      "  → S Cube... ✗\n",
      "\n",
      "======================================================================\n",
      "Scraping: Ram Charan\n",
      "======================================================================\n",
      "  → RRR... ✓\n",
      "  → RC 15... ✓\n",
      "  → Acharya... ✓\n",
      "  → Dhruva... ✓\n",
      "  → Rangasthalam... ✓\n",
      "\n",
      "======================================================================\n",
      "Scraping: Pawan Kalyan\n",
      "======================================================================\n",
      "  → Vakeel Saab... ✓\n",
      "  → Bheemla Nayak... ✓\n",
      "  → Teri Baaton Mein Aisa Uljha Jiya... ✓\n",
      "  → Katamarayudu... ✓\n",
      "  → Sardaar Gabbar Singh... ✓\n",
      "\n",
      "======================================================================\n",
      "SCRAPED DATA SUMMARY\n",
      "======================================================================\n",
      "Salaar                         | Prabhas              | ₹614–702             | 2023\n",
      "Kalki 2898 AD                  | Amitabh Bachchan     | ₹1,042–1,100         | 2024\n",
      "Saaho                          | Prabhas              | est.₹432.4–439       | 2019\n",
      "Baahubali 2                    | Prabhas              | est.₹1,810.60        | 2017\n",
      "Mirchi                         | Prabhas              |                      | \n",
      "Pushpa                         | Allu Arjun           |                      | \n",
      "Ala Vaikunthapurramuloo        | Allu Arjun           | est.₹262–280         | 2020\n",
      "Race Gurram                    | Allu Arjun           | est.₹125-150         | 2014\n",
      "Arjun Reddy                    | Vijay Deverakonda    | ₹51                  | 2017\n",
      "Duvvada Jagannadham            | Allu Arjun           | est.₹150             | 2017\n",
      "Bharat Ane Nenu                | Mahesh Babu          | est.₹225             | 2018\n",
      "Maharshi                       | Mahesh Babu          |                      | \n",
      "Srimanthudu                    | Mahesh Babu          | est.₹200             | 2015\n",
      "Spyder                         | Mahesh Babu          |                      | \n",
      "Businessman                    | Mahesh Babu          |                      | \n",
      "RRR                            | N. T. Rama Rao Jr.   | ₹1,300–1,387         | 2022\n",
      "Devara                         | N. T. Rama Rao Jr.   | est.₹380–521         | 2024\n",
      "Jai Bhim                       | N. T. Rama Rao Jr.   |                      | \n",
      "RRR                            | N. T. Rama Rao Jr.   | ₹1,300–1,387         | 2022\n",
      "RC 15                          | Ram Charan           | est.₹195             | 2025\n",
      "Acharya                        | Ram Charan           |                      | \n",
      "Dhruva                         | Ram Charan           |                      | \n",
      "Rangasthalam                   | Ram Charan           | est.₹216             | 2018\n",
      "Vakeel Saab                    | Pawan Kalyan         | ₹137.65              | 2021\n",
      "Bheemla Nayak                  | Pawan Kalyan         | est.₹160             | 2022\n",
      "Teri Baaton Mein Aisa Uljha Jiya | Shahid Kapoor        | ₹133.64              | 2024\n",
      "Katamarayudu                   | Pawan Kalyan         | est.₹97.5            | 2017\n",
      "Sardaar Gabbar Singh           | Kajal Aggarwal       | est.₹84              | 2016\n",
      "\n",
      "✓ Data saved to tfi_box_office.csv\n",
      "✓ Total movies scraped: 28\n",
      "\n",
      "Done! Check 'tfi_box_office.csv' for complete data.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Top TFI Heroes with their major movies\n",
    "heroes_movies = {\n",
    "    'Prabhas': [\n",
    "        'Salaar',\n",
    "        'Kalki 2898 AD',\n",
    "        'Saaho',\n",
    "        'Baahubali 2',\n",
    "        'Mirchi',\n",
    "    ],\n",
    "    'Allu Arjun': [\n",
    "        'Pushpa',\n",
    "        'Ala Vaikunthapurramuloo',\n",
    "        'Race Gurram',\n",
    "        'Arjun Reddy',\n",
    "        'Duvvada Jagannadham',\n",
    "    ],\n",
    "    'Mahesh Babu': [\n",
    "        'Bharat Ane Nenu',\n",
    "        'Maharshi',\n",
    "        'Srimanthudu',\n",
    "        'Spyder',\n",
    "        'Businessman',\n",
    "    ],\n",
    "    'N. T. Rama Rao Jr.': [\n",
    "        'RRR',\n",
    "        'Devara',\n",
    "        'Aravindha Sametha veera Raghava',\n",
    "        'Jai Lava Kusa',\n",
    "        'Janata Garage',\n",
    "    ],\n",
    "    'Ram Charan': [\n",
    "        'RRR',\n",
    "        'RC 15',\n",
    "        'Acharya',\n",
    "        'Dhruva',\n",
    "        'Rangasthalam',\n",
    "    ],\n",
    "    'Pawan Kalyan': [\n",
    "        'Vakeel Saab',\n",
    "        'Bheemla Nayak',\n",
    "        'Teri Baaton Mein Aisa Uljha Jiya',\n",
    "        'Katamarayudu',\n",
    "        'Sardaar Gabbar Singh',\n",
    "    ],\n",
    "}\n",
    "\n",
    "def clean_box_office(text):\n",
    "    \"\"\"Extract clean box office number from text\"\"\"\n",
    "    if not text:\n",
    "        return ''\n",
    "    text = re.sub(r'\\[.*?\\]', '', text).strip()\n",
    "    text = re.sub(r'crore|Crore', '', text).strip()\n",
    "    return text\n",
    "\n",
    "def extract_first_actor(cast_text):\n",
    "    \"\"\"Extract only the first actor from cast list\"\"\"\n",
    "    if not cast_text:\n",
    "        return ''\n",
    "    cast_text = re.sub(r'\\n', '', cast_text).strip()\n",
    "    actors = re.split(r'[,•]|\\s{2,}', cast_text)\n",
    "    actors = [a.strip() for a in actors if a.strip()]\n",
    "    \n",
    "    if actors:\n",
    "        first_actor = actors[0]\n",
    "        if len(first_actor) > 30:\n",
    "            names = re.findall(r'[A-Z][a-z]*(?:\\s+[A-Z][a-z]*)*', first_actor)\n",
    "            if names:\n",
    "                first_actor = names[0]\n",
    "        return first_actor\n",
    "    return ''\n",
    "\n",
    "def scrape_wikipedia(movie_name):\n",
    "    \"\"\"Scrape Wikipedia for movie data\"\"\"\n",
    "    try:\n",
    "        url = f\"https://en.wikipedia.org/wiki/{movie_name.replace(' ', '_')}\"\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        infobox = soup.find('table', {'class': 'infobox'})\n",
    "        \n",
    "        movie_info = {\n",
    "            'movie_name': movie_name,\n",
    "            'hero_name': '',\n",
    "            'total_WW_cls': '',\n",
    "            'day1_WW_Gross_cr': '',\n",
    "            'verdict': '',\n",
    "            'year_of_release': ''\n",
    "        }\n",
    "        \n",
    "        if infobox:\n",
    "            rows = infobox.find_all('tr')\n",
    "            \n",
    "            for row in rows:\n",
    "                header = row.find('th')\n",
    "                data = row.find('td')\n",
    "                \n",
    "                if header and data:\n",
    "                    header_text = header.get_text(strip=True).lower()\n",
    "                    data_text = data.get_text(strip=True)\n",
    "                    \n",
    "                    if 'starring' in header_text or 'cast' in header_text:\n",
    "                        links = data.find_all('a')\n",
    "                        if links:\n",
    "                            movie_info['hero_name'] = links[0].get_text(strip=True)\n",
    "                        else:\n",
    "                            movie_info['hero_name'] = extract_first_actor(data_text)\n",
    "                    \n",
    "                    elif 'box office' in header_text:\n",
    "                        clean_value = clean_box_office(data_text)\n",
    "                        movie_info['total_WW_cls'] = clean_value\n",
    "                    \n",
    "                    elif 'release' in header_text:\n",
    "                        year_match = re.search(r'\\d{4}', data_text)\n",
    "                        if year_match:\n",
    "                            movie_info['year_of_release'] = year_match.group()\n",
    "        \n",
    "        return movie_info\n",
    "    \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def scrape_all_heroes():\n",
    "    \"\"\"Scrape all heroes and their movies\"\"\"\n",
    "    all_movies_data = []\n",
    "    \n",
    "    for hero, movies in heroes_movies.items():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Scraping: {hero}\")\n",
    "        print('='*70)\n",
    "        \n",
    "        for movie_name in movies:\n",
    "            print(f\"  → {movie_name}...\", end=' ')\n",
    "            \n",
    "            movie_data = scrape_wikipedia(movie_name)\n",
    "            \n",
    "            if movie_data:\n",
    "                # Add hero name if not found\n",
    "                if not movie_data['hero_name']:\n",
    "                    movie_data['hero_name'] = hero\n",
    "                \n",
    "                all_movies_data.append(movie_data)\n",
    "                print(\"✓\")\n",
    "            else:\n",
    "                print(\"✗\")\n",
    "            \n",
    "            time.sleep(0.5)  # Be respectful to servers\n",
    "    \n",
    "    return all_movies_data\n",
    "\n",
    "def save_to_csv(data, filename='tfi_box_office.csv'):\n",
    "    \"\"\"Save data to CSV\"\"\"\n",
    "    if not data:\n",
    "        print(\"\\nNo data to save!\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['movie_name', 'hero_name', 'total_WW_cls', 'day1_WW_Gross_cr', 'verdict', 'year_of_release']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            \n",
    "            writer.writeheader()\n",
    "            for row in data:\n",
    "                writer.writerow(row)\n",
    "        \n",
    "        print(f\"\\n✓ Data saved to {filename}\")\n",
    "        print(f\"✓ Total movies scraped: {len(data)}\")\n",
    "    \n",
    "    except IOError as e:\n",
    "        print(f\"Error saving CSV: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TFI HEROES BOX OFFICE DATA SCRAPER\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    movie_data = scrape_all_heroes()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SCRAPED DATA SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    for movie in movie_data:\n",
    "        print(f\"{movie['movie_name']:30} | {movie['hero_name']:20} | {movie['total_WW_cls']:20} | {movie['year_of_release']}\")\n",
    "    \n",
    "    save_to_csv(movie_data)\n",
    "    \n",
    "    print(\"\\nDone! Check 'tfi_box_office.csv' for complete data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d5d9a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TFI HEROES BOX OFFICE DATA SCRAPER - TEMPLATE\n",
      "================================================================================\n",
      "Just add movie names to 'heroes_movies' dictionary and run!\n",
      "\n",
      "================================================================================\n",
      "HERO: Prabhas\n",
      "================================================================================\n",
      "  Salaar                                   ✓\n",
      "  Kalki 2898 AD                            ✓\n",
      "  Saaho                                    ✓\n",
      "  Baahubali 2                              ✓\n",
      "  Mirchi                                   ✓\n",
      "\n",
      "================================================================================\n",
      "HERO: Allu Arjun\n",
      "================================================================================\n",
      "  Pushpa                                   ✓\n",
      "  Ala Vaikunthapurramuloo                  ✓\n",
      "  Race Gurram                              ✓\n",
      "  Arjun Reddy                              ✓\n",
      "  Duvvada Jagannadham                      ✓\n",
      "\n",
      "================================================================================\n",
      "HERO: Mahesh Babu\n",
      "================================================================================\n",
      "  Bharat Ane Nenu                          ✓\n",
      "  Maharshi                                 ✓\n",
      "  Srimanthudu                              ✓\n",
      "  Spyder                                   ✓\n",
      "  Businessman                              ✓\n",
      "\n",
      "================================================================================\n",
      "HERO: N. T. Rama Rao Jr.\n",
      "================================================================================\n",
      "  RRR                                      ✓\n",
      "  Devara                                   ✓\n",
      "  Jai Bhim                                 ✓\n",
      "\n",
      "================================================================================\n",
      "HERO: Ram Charan\n",
      "================================================================================\n",
      "  RC 15                                    ✓\n",
      "  Acharya                                  ✓\n",
      "  Dhruva                                   ✓\n",
      "  Rangasthalam                             ✓\n",
      "\n",
      "================================================================================\n",
      "HERO: Pawan Kalyan\n",
      "================================================================================\n",
      "  Bheemla Nayak                            ✓\n",
      "  Vakeel Saab                              ✓\n",
      "  Katamarayudu                             ✓\n",
      "\n",
      "================================================================================\n",
      "SCRAPED DATA SUMMARY\n",
      "================================================================================\n",
      "Movie                          Hero                 Total WW        Year  \n",
      "--------------------------------------------------------------------------------\n",
      "Salaar                         Prabhas              614             2023  \n",
      "Kalki 2898 AD                  Amitabh Bachchan     1,042           2024  \n",
      "Saaho                          Prabhas              432.4           2019  \n",
      "Baahubali 2                    Prabhas              1,810.60        2017  \n",
      "Mirchi                         Prabhas                                    \n",
      "Pushpa                         Allu Arjun                                 \n",
      "Ala Vaikunthapurramuloo        Allu Arjun           262             2020  \n",
      "Race Gurram                    Allu Arjun           125             2014  \n",
      "Arjun Reddy                    Vijay Deverakonda    51              2017  \n",
      "Duvvada Jagannadham            Allu Arjun           150             2017  \n",
      "Bharat Ane Nenu                Mahesh Babu          225             2018  \n",
      "Maharshi                       Mahesh Babu                                \n",
      "Srimanthudu                    Mahesh Babu          200             2015  \n",
      "Spyder                         Mahesh Babu                                \n",
      "Businessman                    Mahesh Babu                                \n",
      "RRR                            N. T. Rama Rao Jr.   1,300           2022  \n",
      "Devara                         N. T. Rama Rao Jr.   380             2024  \n",
      "Jai Bhim                       N. T. Rama Rao Jr.                         \n",
      "RC 15                          Ram Charan           195             2025  \n",
      "Acharya                        Ram Charan                                 \n",
      "Dhruva                         Ram Charan                                 \n",
      "Rangasthalam                   Ram Charan           216             2018  \n",
      "Bheemla Nayak                  Pawan Kalyan         160             2022  \n",
      "Vakeel Saab                    Pawan Kalyan         137.65          2021  \n",
      "Katamarayudu                   Pawan Kalyan         97.5            2017  \n",
      "\n",
      "================================================================================\n",
      "✓ Data saved to 'tfi_box_office.csv'\n",
      "✓ Total movies scraped: 25\n",
      "================================================================================\n",
      "\n",
      "Done! Check 'tfi_box_office.csv' for complete data.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Top TFI Heroes with their major movies - Just add movie names here\n",
    "heroes_movies = {\n",
    "    'Prabhas': [\n",
    "        'Salaar',\n",
    "        'Kalki 2898 AD',\n",
    "        'Saaho',\n",
    "        'Baahubali 2',\n",
    "        'Mirchi',\n",
    "    ],\n",
    "    'Allu Arjun': [\n",
    "        'Pushpa',\n",
    "        'Ala Vaikunthapurramuloo',\n",
    "        'Race Gurram',\n",
    "        'Arjun Reddy',\n",
    "        'Duvvada Jagannadham',\n",
    "    ],\n",
    "    'Mahesh Babu': [\n",
    "        'Bharat Ane Nenu',\n",
    "        'Maharshi',\n",
    "        'Srimanthudu',\n",
    "        'Spyder',\n",
    "        'Businessman',\n",
    "    ],\n",
    "    'N. T. Rama Rao Jr.': [\n",
    "        'RRR',\n",
    "        'Devara',\n",
    "        'Jai Bhim',\n",
    "    ],\n",
    "    'Ram Charan': [\n",
    "        'RC 15',\n",
    "        'Acharya',\n",
    "        'Dhruva',\n",
    "        'Rangasthalam',\n",
    "    ],\n",
    "    'Pawan Kalyan': [\n",
    "        'Bheemla Nayak',\n",
    "        'Vakeel Saab',\n",
    "        'Katamarayudu',\n",
    "    ],\n",
    "}\n",
    "\n",
    "def clean_number(text):\n",
    "    \"\"\"Extract numbers from text\"\"\"\n",
    "    if not text:\n",
    "        return ''\n",
    "    # Remove special characters but keep numbers and decimals\n",
    "    numbers = re.findall(r'[\\d,]+\\.?\\d*', text)\n",
    "    return numbers[0] if numbers else ''\n",
    "\n",
    "def scrape_wikipedia(movie_name):\n",
    "    \"\"\"Scrape Wikipedia for movie data\"\"\"\n",
    "    try:\n",
    "        url = f\"https://en.wikipedia.org/wiki/{movie_name.replace(' ', '_')}\"\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        infobox = soup.find('table', {'class': 'infobox'})\n",
    "        \n",
    "        movie_info = {\n",
    "            'movie_name': movie_name,\n",
    "            'hero_name': '',\n",
    "            'total_WW_cls': '',\n",
    "            'day1_WW_Gross_cr': '',\n",
    "            'verdict': '',\n",
    "            'year_of_release': ''\n",
    "        }\n",
    "        \n",
    "        if infobox:\n",
    "            rows = infobox.find_all('tr')\n",
    "            \n",
    "            for row in rows:\n",
    "                header = row.find('th')\n",
    "                data = row.find('td')\n",
    "                \n",
    "                if header and data:\n",
    "                    header_text = header.get_text(strip=True).lower()\n",
    "                    data_text = data.get_text(strip=True)\n",
    "                    \n",
    "                    # Extract hero name from first actor link\n",
    "                    if 'starring' in header_text or 'cast' in header_text:\n",
    "                        links = data.find_all('a')\n",
    "                        if links:\n",
    "                            movie_info['hero_name'] = links[0].get_text(strip=True)\n",
    "                    \n",
    "                    # Extract box office collection\n",
    "                    elif 'box office' in header_text or 'worldwide' in header_text:\n",
    "                        # Try to extract the total collection\n",
    "                        numbers = re.findall(r'₹\\s*([\\d,]+(?:\\.\\d+)?)', data_text)\n",
    "                        if numbers:\n",
    "                            movie_info['total_WW_cls'] = numbers[0]\n",
    "                    \n",
    "                    # Extract release year\n",
    "                    elif 'release' in header_text:\n",
    "                        year_match = re.search(r'\\d{4}', data_text)\n",
    "                        if year_match:\n",
    "                            movie_info['year_of_release'] = year_match.group()\n",
    "        \n",
    "        return movie_info\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"    ! Error: {str(e)[:40]}\")\n",
    "        return None\n",
    "\n",
    "def scrape_imdb(movie_name):\n",
    "    \"\"\"Scrape IMDb for additional data\"\"\"\n",
    "    try:\n",
    "        search_url = f\"https://www.imdb.com/find?q={movie_name.replace(' ', '+')}&s=tt\"\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(search_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Look for movie links\n",
    "        movie_link = soup.find('a', {'href': re.compile(r'/title/tt\\d+/')})\n",
    "        \n",
    "        if movie_link:\n",
    "            movie_url = 'https://www.imdb.com' + movie_link['href']\n",
    "            response = requests.get(movie_url, headers=headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Try to find box office info\n",
    "            page_text = soup.get_text()\n",
    "            \n",
    "            # Look for opening weekend pattern\n",
    "            day1_match = re.search(r'Opening\\s+weekend[:\\s]+\\$?([\\d,]+)', page_text, re.IGNORECASE)\n",
    "            if day1_match:\n",
    "                return {'day1_WW_Gross_cr': day1_match.group(1)}\n",
    "    \n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return {}\n",
    "\n",
    "def scrape_google_search(movie_name):\n",
    "    \"\"\"Search Google for box office info\"\"\"\n",
    "    try:\n",
    "        search_query = f\"{movie_name} box office collection worldwide crore\"\n",
    "        search_url = f\"https://www.google.com/search?q={search_query.replace(' ', '+')}\"\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(search_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        page_text = soup.get_text()\n",
    "        \n",
    "        # Look for collection patterns\n",
    "        collection_patterns = [\n",
    "            r'Total[:\\s]+₹([\\d,]+(?:\\.\\d+)?)\\s*crore',\n",
    "            r'worldwide[:\\s]+₹([\\d,]+(?:\\.\\d+)?)\\s*crore',\n",
    "            r'box office[:\\s]+₹([\\d,]+(?:\\.\\d+)?)\\s*crore',\n",
    "        ]\n",
    "        \n",
    "        for pattern in collection_patterns:\n",
    "            match = re.search(pattern, page_text, re.IGNORECASE)\n",
    "            if match:\n",
    "                return {'total_WW_cls': match.group(1)}\n",
    "        \n",
    "        # Look for verdict\n",
    "        if 'blockbuster' in page_text.lower():\n",
    "            return {'verdict': 'Blockbuster'}\n",
    "        elif 'hit' in page_text.lower():\n",
    "            return {'verdict': 'Hit'}\n",
    "        elif 'flop' in page_text.lower():\n",
    "            return {'verdict': 'Flop'}\n",
    "    \n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return {}\n",
    "\n",
    "def scrape_all_heroes():\n",
    "    \"\"\"Scrape all heroes and their movies\"\"\"\n",
    "    all_movies_data = []\n",
    "    \n",
    "    for hero, movies in heroes_movies.items():\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"HERO: {hero}\")\n",
    "        print('='*80)\n",
    "        \n",
    "        for movie_name in movies:\n",
    "            print(f\"  {movie_name:40}\", end=' ')\n",
    "            \n",
    "            # Scrape Wikipedia (main source)\n",
    "            movie_data = scrape_wikipedia(movie_name)\n",
    "            \n",
    "            if movie_data:\n",
    "                # Add hero name if not found\n",
    "                if not movie_data['hero_name']:\n",
    "                    movie_data['hero_name'] = hero\n",
    "                \n",
    "                # Try to scrape additional data from Google\n",
    "                google_data = scrape_google_search(movie_name)\n",
    "                if google_data:\n",
    "                    for key, value in google_data.items():\n",
    "                        if value and not movie_data[key]:\n",
    "                            movie_data[key] = value\n",
    "                \n",
    "                all_movies_data.append(movie_data)\n",
    "                print(\"✓\")\n",
    "            else:\n",
    "                print(\"✗\")\n",
    "            \n",
    "            time.sleep(1)  # Be respectful to servers\n",
    "    \n",
    "    return all_movies_data\n",
    "\n",
    "def save_to_csv(data, filename='tfi_box_office.csv'):\n",
    "    \"\"\"Save data to CSV\"\"\"\n",
    "    if not data:\n",
    "        print(\"\\nNo data to save!\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['movie_name', 'hero_name', 'total_WW_cls', 'day1_WW_Gross_cr', 'verdict', 'year_of_release']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            \n",
    "            writer.writeheader()\n",
    "            for row in data:\n",
    "                writer.writerow(row)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"✓ Data saved to '{filename}'\")\n",
    "        print(f\"✓ Total movies scraped: {len(data)}\")\n",
    "        print('='*80)\n",
    "    \n",
    "    except IOError as e:\n",
    "        print(f\"Error saving CSV: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TFI HEROES BOX OFFICE DATA SCRAPER - TEMPLATE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Just add movie names to 'heroes_movies' dictionary and run!\")\n",
    "    \n",
    "    movie_data = scrape_all_heroes()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SCRAPED DATA SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Movie':<30} {'Hero':<20} {'Total WW':<15} {'Year':<6}\")\n",
    "    print(\"-\"*80)\n",
    "    for movie in movie_data:\n",
    "        print(f\"{movie['movie_name']:<30} {movie['hero_name']:<20} {movie['total_WW_cls']:<15} {movie['year_of_release']:<6}\")\n",
    "    \n",
    "    save_to_csv(movie_data)\n",
    "    \n",
    "    print(\"\\nDone! Check 'tfi_box_office.csv' for complete data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43492cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TFI HEROES BOX OFFICE DATA SCRAPER - TEMPLATE\n",
      "================================================================================\n",
      "Just add movie names to 'heroes_movies' dictionary and run!\n",
      "\n",
      "================================================================================\n",
      "HERO: Jr NTR\n",
      "================================================================================\n",
      "  Student No: 1                            ✓\n",
      "  Subbu                                    ✓\n",
      "  Ninnu Choodalani                         ✓\n",
      "  Aadi                                     ✓\n",
      "  Santosham                                ✓\n",
      "  Simhadri                                 ✓\n",
      "  Kadha Parthu                                 ! Error: 404 Client Error: Not Found for url: htt\n",
      "✗\n",
      "  Shivamani                                ✓\n",
      "  Malik Ram                                ✓\n",
      "  Andhrodu                                     ! Error: 404 Client Error: Not Found for url: htt\n",
      "✗\n",
      "  Rakhi                                    ✓\n",
      "  Ashok                                    ✓\n",
      "  Yamadonga                                ✓\n",
      "  Kantri                                   ✓\n",
      "  Adhurs                                   ✓\n",
      "  Brindavanam                              ✓\n",
      "  Shakti                                   ✓\n",
      "  Oosaravelli                              ✓\n",
      "  Dammu                                    ✓\n",
      "  Janatha Garage                           ✓\n",
      "  Temper                                   ✓\n",
      "  Nannaku Prematho                         ✓\n",
      "  Jai Lava Kusa                            ✓\n",
      "  Aravinda Sametha                         ✓\n",
      "  RRR                                      ✓\n",
      "  Devara                                   ✓\n",
      "\n",
      "================================================================================\n",
      "HERO: Mahesh Babu\n",
      "================================================================================\n",
      "  Rajakumarudu                             ✓\n",
      "  Yuvarajuv                                    ! Error: 404 Client Error: Not Found for url: htt\n",
      "✗\n",
      "  Vamsi                                    ✓\n",
      "  Okkadu                                   ✓\n",
      "  Nijam                                    ✓\n",
      "  Neeku Nenu Naku Nuvvu                        ! Error: 404 Client Error: Not Found for url: htt\n",
      "✗\n",
      "  Tagore                                   ✓\n",
      "  Athadu                                   ✓\n",
      "  Pokiri                                   ✓\n",
      "  Sainikudu                                ✓\n",
      "  Dhookudu                                     ! Error: 404 Client Error: Not Found for url: htt\n",
      "✗\n",
      "  Businessman                              ✓\n",
      "  Seethamma Vakitlo Sirimalle Chettu       ✓\n",
      "  1 Nenokkadine                            ✓\n",
      "  Aagadu                                   ✓\n",
      "  Srimanthudu                              ✓\n",
      "  Bharat Ane Nenu                          ✓\n",
      "  Maharshi                                 ✓\n",
      "  Sarileru Neekevvaru                      ✓\n",
      "  Sarkaru Vaari Paata                      ✓\n",
      "  Guntur Kaaram                            ✓\n",
      "  Spyder                                   ✓\n",
      "\n",
      "================================================================================\n",
      "HERO: Pawan Kalyan\n",
      "================================================================================\n",
      "  Akkada Ammayi Ikkada Abbayi              ✓\n",
      "  Tholi Prema                              ✓\n",
      "  Thammudu                                 ✓\n",
      "  Badri                                    ✓\n",
      "  Kushi                                    ✓\n",
      "  Johnny                                   ✓\n",
      "  Gudumba Shankar                          ✓\n",
      "  Balu ABCDEFG                             ✓\n",
      "  Bangaram                                 ✓\n",
      "  Annavaram                                ✓\n",
      "  Shankar Dada MBBS                        ✓\n",
      "  Katamarayudu                             ✓\n",
      "  Agnyaatavaasi                                ! Error: 404 Client Error: Not Found for url: htt\n",
      "✗\n",
      "  Vakeel Saab                              ✓\n",
      "  Bheemla Nayak                            ✓\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 250\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJust add movie names to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheroes_movies\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m dictionary and run!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m movie_data \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_all_heroes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSCRAPED DATA SUMMARY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 217\u001b[0m, in \u001b[0;36mscrape_all_heroes\u001b[1;34m()\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✗\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 217\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Be respectful to servers\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_movies_data\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Top TFI Heroes with their complete filmography - Just add movie names here\n",
    "heroes_movies = {\n",
    "    'Jr NTR': [\n",
    "        'Student No: 1', 'Subbu', 'Ninnu Choodalani', 'Aadi', 'Santosham', 'Simhadri',\n",
    "        'Kadha Parthu', 'Shivamani', 'Malik Ram', 'Andhrodu', 'Rakhi', 'Ashok', 'Yamadonga',\n",
    "        'Kantri', 'Adhurs', 'Brindavanam', 'Shakti', 'Oosaravelli', 'Dammu', 'Janatha Garage',\n",
    "        'Temper', 'Nannaku Prematho', 'Jai Lava Kusa', 'Aravinda Sametha', 'RRR', 'Devara'\n",
    "    ],\n",
    "    'Mahesh Babu': [\n",
    "        'Rajakumarudu', 'Yuvarajuv', 'Vamsi', 'Okkadu', 'Nijam', 'Neeku Nenu Naku Nuvvu',\n",
    "        'Tagore', 'Athadu', 'Pokiri', 'Sainikudu', 'Dhookudu', 'Businessman', 'Seethamma Vakitlo Sirimalle Chettu',\n",
    "        '1 Nenokkadine', 'Aagadu', 'Srimanthudu', 'Bharat Ane Nenu', 'Maharshi', 'Sarileru Neekevvaru',\n",
    "        'Sarkaru Vaari Paata', 'Guntur Kaaram', 'Spyder'\n",
    "    ],\n",
    "    'Pawan Kalyan': [\n",
    "        'Akkada Ammayi Ikkada Abbayi', 'Tholi Prema', 'Thammudu', 'Badri', 'Kushi', 'Johnny',\n",
    "        'Gudumba Shankar', 'Balu ABCDEFG', 'Bangaram', 'Annavaram', 'Shankar Dada MBBS', 'Katamarayudu',\n",
    "        'Agnyaatavaasi', 'Vakeel Saab', 'Bheemla Nayak', 'They Call Him OG', 'Attarintiki Daredi',\n",
    "        'Gabbar Singh', 'Cameraman Gangatho Rambabu', 'Teen Maar'\n",
    "    ],\n",
    "    'Allu Arjun': [\n",
    "        'Gangotri', 'Arya', 'Desamudu', 'Happy', 'Bunny', 'Arya 2', 'Vedam', 'Badrinath',\n",
    "        'Julai', 'Race Gurram', 'S/O Satyamurthy', 'Sarainodu', 'Duvvada Jagannadham', 'Naa Peru Surya',\n",
    "        'Ala Vaikunthapurramulo', 'Pushpa: The Rise', 'Pushpa 2: The Rule', 'Parugu', 'Rudhramadevi'\n",
    "    ],\n",
    "    'Ram Charan': [\n",
    "        'Chirutha', 'Magadheera', 'Orange', 'Leader', 'Racha', 'Naayak', 'Zanjeer', 'Yevadu',\n",
    "        'Govindudu Andarivadele', 'Dhruva', 'Rangasthalam', 'Vinaya Vidheya Rama', 'Acharya', 'RRR',\n",
    "        'Bruce Lee', 'Toofaan', 'Game Changer'\n",
    "    ],\n",
    "    'Prabhas': [\n",
    "        'Eeswar', 'Raghavendra', 'Varsham', 'Adavi Ramudu', 'Chatrapathi', 'Bujjigadu', 'Billa', 'Mirchi',\n",
    "        'Baahubali: The Beginning', 'Baahubali 2: The Conclusion', 'Saaho', 'Radhe Shyam', 'Adipurush',\n",
    "        'Salaar', 'Kalki 2898 AD', 'Darling', 'Mr. Perfect'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def clean_number(text):\n",
    "    \"\"\"Extract numbers from text\"\"\"\n",
    "    if not text:\n",
    "        return ''\n",
    "    # Remove special characters but keep numbers and decimals\n",
    "    numbers = re.findall(r'[\\d,]+\\.?\\d*', text)\n",
    "    return numbers[0] if numbers else ''\n",
    "\n",
    "def scrape_wikipedia(movie_name):\n",
    "    \"\"\"Scrape Wikipedia for movie data\"\"\"\n",
    "    try:\n",
    "        url = f\"https://en.wikipedia.org/wiki/{movie_name.replace(' ', '_')}\"\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        infobox = soup.find('table', {'class': 'infobox'})\n",
    "        \n",
    "        movie_info = {\n",
    "            'movie_name': movie_name,\n",
    "            'hero_name': '',\n",
    "            'total_WW_cls': '',\n",
    "            'day1_WW_Gross_cr': '',\n",
    "            'verdict': '',\n",
    "            'year_of_release': ''\n",
    "        }\n",
    "        \n",
    "        if infobox:\n",
    "            rows = infobox.find_all('tr')\n",
    "            \n",
    "            for row in rows:\n",
    "                header = row.find('th')\n",
    "                data = row.find('td')\n",
    "                \n",
    "                if header and data:\n",
    "                    header_text = header.get_text(strip=True).lower()\n",
    "                    data_text = data.get_text(strip=True)\n",
    "                    \n",
    "                    # Extract hero name from first actor link\n",
    "                    if 'starring' in header_text or 'cast' in header_text:\n",
    "                        links = data.find_all('a')\n",
    "                        if links:\n",
    "                            movie_info['hero_name'] = links[0].get_text(strip=True)\n",
    "                    \n",
    "                    # Extract box office collection\n",
    "                    elif 'box office' in header_text or 'worldwide' in header_text:\n",
    "                        # Try to extract the total collection\n",
    "                        numbers = re.findall(r'₹\\s*([\\d,]+(?:\\.\\d+)?)', data_text)\n",
    "                        if numbers:\n",
    "                            movie_info['total_WW_cls'] = numbers[0]\n",
    "                    \n",
    "                    # Extract release year\n",
    "                    elif 'release' in header_text:\n",
    "                        year_match = re.search(r'\\d{4}', data_text)\n",
    "                        if year_match:\n",
    "                            movie_info['year_of_release'] = year_match.group()\n",
    "        \n",
    "        return movie_info\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"    ! Error: {str(e)[:40]}\")\n",
    "        return None\n",
    "\n",
    "def scrape_imdb(movie_name):\n",
    "    \"\"\"Scrape IMDb for additional data\"\"\"\n",
    "    try:\n",
    "        search_url = f\"https://www.imdb.com/find?q={movie_name.replace(' ', '+')}&s=tt\"\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(search_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Look for movie links\n",
    "        movie_link = soup.find('a', {'href': re.compile(r'/title/tt\\d+/')})\n",
    "        \n",
    "        if movie_link:\n",
    "            movie_url = 'https://www.imdb.com' + movie_link['href']\n",
    "            response = requests.get(movie_url, headers=headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Try to find box office info\n",
    "            page_text = soup.get_text()\n",
    "            \n",
    "            # Look for opening weekend pattern\n",
    "            day1_match = re.search(r'Opening\\s+weekend[:\\s]+\\$?([\\d,]+)', page_text, re.IGNORECASE)\n",
    "            if day1_match:\n",
    "                return {'day1_WW_Gross_cr': day1_match.group(1)}\n",
    "    \n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return {}\n",
    "\n",
    "def scrape_google_search(movie_name):\n",
    "    \"\"\"Search Google for box office info\"\"\"\n",
    "    try:\n",
    "        search_query = f\"{movie_name} box office collection worldwide crore\"\n",
    "        search_url = f\"https://www.google.com/search?q={search_query.replace(' ', '+')}\"\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(search_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        page_text = soup.get_text()\n",
    "        \n",
    "        # Look for collection patterns\n",
    "        collection_patterns = [\n",
    "            r'Total[:\\s]+₹([\\d,]+(?:\\.\\d+)?)\\s*crore',\n",
    "            r'worldwide[:\\s]+₹([\\d,]+(?:\\.\\d+)?)\\s*crore',\n",
    "            r'box office[:\\s]+₹([\\d,]+(?:\\.\\d+)?)\\s*crore',\n",
    "        ]\n",
    "        \n",
    "        for pattern in collection_patterns:\n",
    "            match = re.search(pattern, page_text, re.IGNORECASE)\n",
    "            if match:\n",
    "                return {'total_WW_cls': match.group(1)}\n",
    "        \n",
    "        # Look for verdict\n",
    "        if 'blockbuster' in page_text.lower():\n",
    "            return {'verdict': 'Blockbuster'}\n",
    "        elif 'hit' in page_text.lower():\n",
    "            return {'verdict': 'Hit'}\n",
    "        elif 'flop' in page_text.lower():\n",
    "            return {'verdict': 'Flop'}\n",
    "    \n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return {}\n",
    "\n",
    "def scrape_all_heroes():\n",
    "    \"\"\"Scrape all heroes and their movies\"\"\"\n",
    "    all_movies_data = []\n",
    "    \n",
    "    for hero, movies in heroes_movies.items():\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"HERO: {hero}\")\n",
    "        print('='*80)\n",
    "        \n",
    "        for movie_name in movies:\n",
    "            print(f\"  {movie_name:40}\", end=' ')\n",
    "            \n",
    "            # Scrape Wikipedia (main source)\n",
    "            movie_data = scrape_wikipedia(movie_name)\n",
    "            \n",
    "            if movie_data:\n",
    "                # Add hero name if not found\n",
    "                if not movie_data['hero_name']:\n",
    "                    movie_data['hero_name'] = hero\n",
    "                \n",
    "                # Try to scrape additional data from Google\n",
    "                google_data = scrape_google_search(movie_name)\n",
    "                if google_data:\n",
    "                    for key, value in google_data.items():\n",
    "                        if value and not movie_data[key]:\n",
    "                            movie_data[key] = value\n",
    "                \n",
    "                all_movies_data.append(movie_data)\n",
    "                print(\"✓\")\n",
    "            else:\n",
    "                print(\"✗\")\n",
    "            \n",
    "            time.sleep(1)  # Be respectful to servers\n",
    "    \n",
    "    return all_movies_data\n",
    "\n",
    "def save_to_csv(data, filename='tfi_box_office.csv'):\n",
    "    \"\"\"Save data to CSV\"\"\"\n",
    "    if not data:\n",
    "        print(\"\\nNo data to save!\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['movie_name', 'hero_name', 'total_WW_cls', 'day1_WW_Gross_cr', 'verdict', 'year_of_release']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            \n",
    "            writer.writeheader()\n",
    "            for row in data:\n",
    "                writer.writerow(row)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"✓ Data saved to '{filename}'\")\n",
    "        print(f\"✓ Total movies scraped: {len(data)}\")\n",
    "        print('='*80)\n",
    "    \n",
    "    except IOError as e:\n",
    "        print(f\"Error saving CSV: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TFI HEROES BOX OFFICE DATA SCRAPER - TEMPLATE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Just add movie names to 'heroes_movies' dictionary and run!\")\n",
    "    \n",
    "    movie_data = scrape_all_heroes()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SCRAPED DATA SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Movie':<30} {'Hero':<20} {'Total WW':<15} {'Year':<6}\")\n",
    "    print(\"-\"*80)\n",
    "    for movie in movie_data:\n",
    "        print(f\"{movie['movie_name']:<30} {movie['hero_name']:<20} {movie['total_WW_cls']:<15} {movie['year_of_release']:<6}\")\n",
    "    \n",
    "    save_to_csv(movie_data)\n",
    "    \n",
    "    print(\"\\nDone! Check 'tfi_box_office.csv' for complete data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7e7c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TFI HEROES BOX OFFICE DATA SCRAPER (Multi-Source)\n",
      "================================================================================\n",
      "Scraping from: Sacnilk, Andhra Box Office, Wikipedia...\n",
      "\n",
      "================================================================================\n",
      "[1/6] HERO: Jr NTR (24 movies)\n",
      "================================================================================\n",
      "  [1/24] Student No: 1                            [SAW] ✓\n",
      "  [2/24] Subbu                                    [SAW] ✓\n",
      "  [3/24] Ninnu Choodalani                         [SAW] ✓\n",
      "  [4/24] Aadi                                     [SAW] ✓\n",
      "  [5/24] Santosham                                [SAW] ✓\n",
      "  [6/24] Simhadri                                 [SAW] ✓\n",
      "  [7/24] Kadha Parthu                             [SAW] ✓\n",
      "  [8/24] Shivamani                                [SAW] ✓\n",
      "  [9/24] Andhrodu                                 [SAW] ✓\n",
      "  [10/24] Rakhi                                    [SAW] ✓\n",
      "  [11/24] Ashok                                    [SAW] ✓\n",
      "  [12/24] Yamadonga                                [SAW] ✓\n",
      "  [13/24] Kantri                                   [SAW] ✓\n",
      "  [14/24] Adhurs                                   [SAW] ✓\n",
      "  [15/24] Brindavanam                              [SAW] ✓\n",
      "  [16/24] Oosaravelli                              [SAW] ✓\n",
      "  [17/24] Dammu                                    [SAW] ✓\n",
      "  [18/24] Janatha Garage                           [SAW] ✓\n",
      "  [19/24] Temper                                   [SAW] ✓\n",
      "  [20/24] Nannaku Prematho                         [SAW] ✓\n",
      "  [21/24] Jai Lava Kusa                            [SAW] ✓\n",
      "  [22/24] Aravinda Sametha                         [SAW] ✓\n",
      "  [23/24] RRR                                      [SAW] ✓\n",
      "  [24/24] Devara                                   [SAW] ✓\n",
      "  ✓ Processed 24/24 movies\n",
      "\n",
      "================================================================================\n",
      "[2/6] HERO: Mahesh Babu (22 movies)\n",
      "================================================================================\n",
      "  [1/22] Rajakumarudu                             [SAW] ✓\n",
      "  [2/22] Yuvarajuv                                [SAW] ✓\n",
      "  [3/22] Vamsi                                    [SAW] ✓\n",
      "  [4/22] Okkadu                                   [SAW] ✓\n",
      "  [5/22] Nijam                                    [SAW] ✓\n",
      "  [6/22] Neeku Nenu Naku Nuvvu                    [SAW] ✓\n",
      "  [7/22] Tagore                                   [SAW] ✓\n",
      "  [8/22] Athadu                                   [SAW] ✓\n",
      "  [9/22] Pokiri                                   [SAW] ✓\n",
      "  [10/22] Sainikudu                                [SAW] ✓\n",
      "  [11/22] Dhookudu                                 [SAW] ✓\n",
      "  [12/22] Businessman                              [SAW] ✓\n",
      "  [13/22] Seethamma Vakitlo Sirimalle Chettu       [SAW] ✓\n",
      "  [14/22] 1 Nenokkadine                            [SAW] ✓\n",
      "  [15/22] Aagadu                                   [SAW] ✓\n",
      "  [16/22] Srimanthudu                              [SAW] ✓\n",
      "  [17/22] Bharat Ane Nenu                          [SAW] ✓\n",
      "  [18/22] Maharshi                                 [SAW] ✓\n",
      "  [19/22] Sarileru Neekevvaru                      [SAW] ✓\n",
      "  [20/22] Sarkaru Vaari Paata                      [SAW] ✓\n",
      "  [21/22] Guntur Kaaram                            [SAW] ✓\n",
      "  [22/22] Spyder                                   [SAW] ✓\n",
      "  ✓ Processed 22/22 movies\n",
      "\n",
      "================================================================================\n",
      "[3/6] HERO: Pawan Kalyan (20 movies)\n",
      "================================================================================\n",
      "  [1/20] Akkada Ammayi Ikkada Abbayi              [SAW] ✓\n",
      "  [2/20] Tholi Prema                              [SAW] ✓\n",
      "  [3/20] Thammudu                                 [SAW] ✓\n",
      "  [4/20] Badri                                    [SAW] ✓\n",
      "  [5/20] Kushi                                    [SAW] ✓\n",
      "  [6/20] Johnny                                   [SAW] ✓\n",
      "  [7/20] Gudumba Shankar                          [SAW] ✓\n",
      "  [8/20] Balu ABCDEFG                             [SAW] ✓\n",
      "  [9/20] Bangaram                                 [SAW] ✓\n",
      "  [10/20] Annavaram                                [SAW] ✓\n",
      "  [11/20] Shankar Dada MBBS                        [SAW] ✓\n",
      "  [12/20] Katamarayudu                             [SAW] ✓\n",
      "  [13/20] Agnyaatavaasi                            [SAW] ✓\n",
      "  [14/20] Vakeel Saab                              [SAW] ✓\n",
      "  [15/20] Bheemla Nayak                            [SAW] ✓\n",
      "  [16/20] They Call Him OG                         [SAW] ✓\n",
      "  [17/20] Attarintiki Daredi                       [SAW] ✓\n",
      "  [18/20] Gabbar Singh                             [SAW] ✓\n",
      "  [19/20] Cameraman Gangatho Rambabu               [SAW] ✓\n",
      "  [20/20] Teen Maar                                [SAW] ✓\n",
      "  ✓ Processed 20/20 movies\n",
      "\n",
      "================================================================================\n",
      "[4/6] HERO: Allu Arjun (19 movies)\n",
      "================================================================================\n",
      "  [1/19] Gangotri                                 [SAW] ✓\n",
      "  [2/19] Arya                                     [SAW] ✓\n",
      "  [3/19] Desamudu                                 [SAW] ✓\n",
      "  [4/19] Happy                                    [SAW] ✓\n",
      "  [5/19] Bunny                                    [SAW] ✓\n",
      "  [6/19] Arya 2                                   [SAW] ✓\n",
      "  [7/19] Vedam                                    [SAW] ✓\n",
      "  [8/19] Badrinath                                [SAW] ✓\n",
      "  [9/19] Julai                                    [SAW] ✓\n",
      "  [10/19] Race Gurram                              [SAW] ✓\n",
      "  [11/19] S/O Satyamurthy                          [SAW] ✓\n",
      "  [12/19] Sarainodu                                [SAW] ✓\n",
      "  [13/19] Duvvada Jagannadham                      [SAW] ✓\n",
      "  [14/19] Naa Peru Surya                           [SAW] ✓\n",
      "  [15/19] Ala Vaikunthapurramulo                   [SAW] ✓\n",
      "  [16/19] Pushpa: The Rise                         [SAW] ✓\n",
      "  [17/19] Pushpa 2: The Rule                       [SAW] ✓\n",
      "  [18/19] Parugu                                   [SAW] ✓\n",
      "  [19/19] Rudhramadevi                             [SAW] ✓\n",
      "  ✓ Processed 19/19 movies\n",
      "\n",
      "================================================================================\n",
      "[5/6] HERO: Ram Charan (17 movies)\n",
      "================================================================================\n",
      "  [1/17] Chirutha                                 [SAW] ✓\n",
      "  [2/17] Magadheera                               [SAW] ✓\n",
      "  [3/17] Orange                                   [SAW] ✓\n",
      "  [4/17] Leader                                   [SAW] ✓\n",
      "  [5/17] Racha                                    [SAW] ✓\n",
      "  [6/17] Naayak                                   [SAW] ✓\n",
      "  [7/17] Zanjeer                                  [SAW] ✓\n",
      "  [8/17] Yevadu                                   [SAW] ✓\n",
      "  [9/17] Govindudu Andarivadele                   [SAW] ✓\n",
      "  [10/17] Dhruva                                   [SAW] ✓\n",
      "  [11/17] Rangasthalam                             [SAW] ✓\n",
      "  [12/17] Vinaya Vidheya Rama                      [SAW] ✓\n",
      "  [13/17] Acharya                                  [SAW] ✓\n",
      "  [14/17] RRR                                      [SAW] ✓\n",
      "  [15/17] Bruce Lee                                [SAW] ✓\n",
      "  [16/17] Toofaan                                  [SAW] ✓\n",
      "  [17/17] Game Changer                             [SAW] ✓\n",
      "  ✓ Processed 17/17 movies\n",
      "\n",
      "================================================================================\n",
      "[6/6] HERO: Prabhas (17 movies)\n",
      "================================================================================\n",
      "  [1/17] Eeswar                                   [SAW] ✓\n",
      "  [2/17] Raghavendra                              [SAW] ✓\n",
      "  [3/17] Varsham                                  [SAW] ✓\n",
      "  [4/17] Adavi Ramudu                             [SAW] ✓\n",
      "  [5/17] Chatrapathi                              [SAW] ✓\n",
      "  [6/17] Bujjigadu                                [SAW] ✓\n",
      "  [7/17] Billa                                    [SAW] ✓\n",
      "  [8/17] Mirchi                                   [SAW] ✓\n",
      "  [9/17] Baahubali: The Beginning                 [SAW] ✓\n",
      "  [10/17] Baahubali 2: The Conclusion              [SAW] ✓\n",
      "  [11/17] Saaho                                    [SAW] ✓\n",
      "  [12/17] Radhe Shyam                              [SAW] ✓\n",
      "  [13/17] Adipurush                                [SAW] ✓\n",
      "  [14/17] Salaar                                   [SAW] ✓\n",
      "  [15/17] Kalki 2898 AD                            [SAW] ✓\n",
      "  [16/17] Darling                                  [SAW] ✓\n",
      "  [17/17] Mr. Perfect                              [SAW] ✓\n",
      "  ✓ Processed 17/17 movies\n",
      "\n",
      "================================================================================\n",
      "✓ Data saved to 'tfi_box_office.csv'\n",
      "✓ Total movies: 119\n",
      "================================================================================\n",
      "\n",
      "Done! Check 'tfi_box_office.csv' for complete data.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Top TFI Heroes with their complete filmography\n",
    "heroes_movies = {\n",
    "    'Jr NTR': [\n",
    "        'Student No: 1', 'Subbu', 'Ninnu Choodalani', 'Aadi', 'Santosham', 'Simhadri',\n",
    "        'Yamadonga', 'Kantri', 'Adhurs', 'Oosaravelli', 'Dammu', 'Janatha Garage',\n",
    "        'Nannaku Prematho', 'Jai Lava Kusa', 'Aravinda Sametha', 'RRR', 'Devara'\n",
    "    ],\n",
    "    'Mahesh Babu': [\n",
    "        'Okkadu', 'Pokiri', 'Srimanthudu', 'Bharat Ane Nenu', 'Maharshi', \n",
    "        'Sarileru Neekevvaru', 'Sarkaru Vaari Paata', 'Guntur Kaaram'\n",
    "    ],\n",
    "    'Pawan Kalyan': [\n",
    "        'Katamarayudu', 'Vakeel Saab', 'Bheemla Nayak', 'They Call Him OG', \n",
    "        'Attarintiki Daredi', 'Teen Maar'\n",
    "    ],\n",
    "    'Allu Arjun': [\n",
    "        'Race Gurram', 'S/O Satyamurthy', 'Sarainodu', 'Duvvada Jagannadham', \n",
    "        'Naa Peru Surya', 'Pushpa: The Rise', 'Pushpa 2: The Rule'\n",
    "    ],\n",
    "    'Ram Charan': [\n",
    "        'Magadheera', 'Yevadu', 'Govindudu Andarivadele', 'Rangasthalam', 'RRR',\n",
    "        'Vinaya Vidheya Rama', 'Acharya', 'Game Changer'\n",
    "    ],\n",
    "    'Prabhas': [\n",
    "        'Baahubali: The Beginning', 'Baahubali 2: The Conclusion', 'Saaho', \n",
    "        'Radhe Shyam', 'Adipurush', 'Salaar', 'Kalki 2898 AD'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def scrape_sacnilk_movie(movie_name):\n",
    "    \"\"\"Scrape Sacnilk for complete movie data\"\"\"\n",
    "    try:\n",
    "        # Construct Sacnilk URL\n",
    "        url = f\"https://www.sacnilk.com/news/{movie_name.replace(' ', '_')}_Box_Office_Collection\"\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        page_text = soup.get_text()\n",
    "        \n",
    "        movie_data = {\n",
    "            'total_WW_cls': '',\n",
    "            'day1_WW_Gross_cr': '',\n",
    "            'verdict': '',\n",
    "            'year_of_release': ''\n",
    "        }\n",
    "        \n",
    "        # Extract Worldwide Collection\n",
    "        patterns = [\n",
    "            r'Worldwide[:\\s]+Collection[:\\s]*₹\\s*([\\d,]+(?:\\.\\d+)?)\\s*Cr',\n",
    "            r'Worldwide[:\\s]+₹\\s*([\\d,]+(?:\\.\\d+)?)\\s*(?:Cr|Crore)',\n",
    "            r'Total[:\\s]+Worldwide[:\\s]+₹\\s*([\\d,]+(?:\\.\\d+)?)',\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, page_text, re.IGNORECASE)\n",
    "            if match:\n",
    "                movie_data['total_WW_cls'] = match.group(1)\n",
    "                break\n",
    "        \n",
    "        # Extract Day 1 Worldwide Collection\n",
    "        day1_patterns = [\n",
    "            r'1 Day Worldwide Collection[:\\s]*₹\\s*([\\d,]+(?:\\.\\d+)?)\\s*Cr',\n",
    "            r'Opening Day[:\\s]+Worldwide[:\\s]*₹\\s*([\\d,]+(?:\\.\\d+)?)',\n",
    "            r'Day 1[:\\s]+Worldwide[:\\s]*₹\\s*([\\d,]+(?:\\.\\d+)?)',\n",
    "        ]\n",
    "        \n",
    "        for pattern in day1_patterns:\n",
    "            match = re.search(pattern, page_text, re.IGNORECASE)\n",
    "            if match:\n",
    "                movie_data['day1_WW_Gross_cr'] = match.group(1)\n",
    "                break\n",
    "        \n",
    "        # Extract Verdict\n",
    "        if 'Verdict:' in page_text or 'verdict:' in page_text:\n",
    "            verdict_match = re.search(r'Verdict[:\\s]+(\\w+)', page_text, re.IGNORECASE)\n",
    "            if verdict_match:\n",
    "                verdict = verdict_match.group(1).strip()\n",
    "                if verdict.lower() in ['blockbuster', 'superhit', 'hit', 'flop']:\n",
    "                    movie_data['verdict'] = verdict.capitalize()\n",
    "        \n",
    "        # Extract Release Year\n",
    "        if 'Release Date:' in page_text:\n",
    "            year_match = re.search(r'Release Date[:\\s]+\\w+\\s+\\d+[a-z]*\\s+(20\\d{2})', page_text, re.IGNORECASE)\n",
    "            if year_match:\n",
    "                movie_data['year_of_release'] = year_match.group(1)\n",
    "        \n",
    "        return movie_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {}\n",
    "\n",
    "def scrape_wikipedia(movie_name):\n",
    "    \"\"\"Scrape Wikipedia for hero and year data\"\"\"\n",
    "    try:\n",
    "        url = f\"https://en.wikipedia.org/wiki/{movie_name.replace(' ', '_')}\"\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        infobox = soup.find('table', {'class': 'infobox'})\n",
    "        \n",
    "        movie_info = {\n",
    "            'hero_name': '',\n",
    "            'year_of_release': ''\n",
    "        }\n",
    "        \n",
    "        if infobox:\n",
    "            rows = infobox.find_all('tr')\n",
    "            \n",
    "            for row in rows:\n",
    "                header = row.find('th')\n",
    "                data = row.find('td')\n",
    "                \n",
    "                if header and data:\n",
    "                    header_text = header.get_text(strip=True).lower()\n",
    "                    data_text = data.get_text(strip=True)\n",
    "                    \n",
    "                    if 'starring' in header_text or 'cast' in header_text:\n",
    "                        links = data.find_all('a')\n",
    "                        if links:\n",
    "                            movie_info['hero_name'] = links[0].get_text(strip=True)\n",
    "                    \n",
    "                    elif 'release' in header_text:\n",
    "                        year_match = re.search(r'\\d{4}', data_text)\n",
    "                        if year_match:\n",
    "                            movie_info['year_of_release'] = year_match.group()\n",
    "        \n",
    "        return movie_info\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {}\n",
    "\n",
    "def scrape_all_heroes():\n",
    "    \"\"\"Scrape all heroes and their movies\"\"\"\n",
    "    all_movies_data = []\n",
    "    total_heroes = len(heroes_movies)\n",
    "    current_hero = 0\n",
    "    \n",
    "    for hero, movies in heroes_movies.items():\n",
    "        current_hero += 1\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"[{current_hero}/{total_heroes}] HERO: {hero} ({len(movies)} movies)\")\n",
    "        print('='*80)\n",
    "        \n",
    "        for idx, movie_name in enumerate(movies, 1):\n",
    "            print(f\"  [{idx}/{len(movies)}] {movie_name:40}\", end=' ')\n",
    "            \n",
    "            # Scrape Sacnilk for box office data\n",
    "            sacnilk_data = scrape_sacnilk_movie(movie_name)\n",
    "            \n",
    "            # Scrape Wikipedia for hero and year\n",
    "            wiki_data = scrape_wikipedia(movie_name)\n",
    "            \n",
    "            # Merge data\n",
    "            movie_info = {\n",
    "                'movie_name': movie_name,\n",
    "                'hero_name': wiki_data.get('hero_name') or hero,\n",
    "                'total_WW_cls': sacnilk_data.get('total_WW_cls', ''),\n",
    "                'day1_WW_Gross_cr': sacnilk_data.get('day1_WW_Gross_cr', ''),\n",
    "                'verdict': sacnilk_data.get('verdict', ''),\n",
    "                'year_of_release': sacnilk_data.get('year_of_release') or wiki_data.get('year_of_release', '')\n",
    "            }\n",
    "            \n",
    "            all_movies_data.append(movie_info)\n",
    "            print(\"✓\")\n",
    "            \n",
    "            time.sleep(0.6)  # Respectful delay\n",
    "        \n",
    "        print(f\"  ✓ Processed {len(movies)} movies\")\n",
    "    \n",
    "    return all_movies_data\n",
    "\n",
    "def save_to_csv(data, filename='tfi_box_office.csv'):\n",
    "    \"\"\"Save data to CSV\"\"\"\n",
    "    if not data:\n",
    "        print(\"\\nNo data to save!\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['movie_name', 'hero_name', 'total_WW_cls', 'day1_WW_Gross_cr', 'verdict', 'year_of_release']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            \n",
    "            writer.writeheader()\n",
    "            for row in data:\n",
    "                writer.writerow(row)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"✓ Data saved to '{filename}'\")\n",
    "        print(f\"✓ Total movies: {len(data)}\")\n",
    "        \n",
    "        # Stats\n",
    "        with_verdict = sum(1 for m in data if m['verdict'])\n",
    "        with_day1 = sum(1 for m in data if m['day1_WW_Gross_cr'])\n",
    "        \n",
    "        print(f\"✓ Movies with verdict: {with_verdict}\")\n",
    "        print(f\"✓ Movies with day1 data: {with_day1}\")\n",
    "        print('='*80)\n",
    "    \n",
    "    except IOError as e:\n",
    "        print(f\"Error saving CSV: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TFI HEROES BOX OFFICE DATA SCRAPER (Sacnilk + Wikipedia)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Scraping from Sacnilk.com for box office data...\")\n",
    "    \n",
    "    movie_data = scrape_all_heroes()\n",
    "    save_to_csv(movie_data)\n",
    "    \n",
    "    print(\"\\nDone! Check 'tfi_box_office.csv' for complete data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba309eb7",
   "metadata": {},
   "source": [
    "GROK(WORKING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89da05a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TFI HEROES BOX OFFICE DATA SCRAPER (Improved 2025 Version)\n",
      "================================================================================\n",
      "Sources: Sacnilk (primary), AndhraBoxOffice, Wikipedia\n",
      "\n",
      "================================================================================\n",
      "[1/6] HERO: Jr NTR (12 movies)\n",
      "================================================================================\n",
      "  [1/12] Kantri                                   [S"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AW] ✓\n",
      "  [2/12] Adhurs                                   [SAW] ✓\n",
      "  [3/12] Brindavanam                              [SAW] ✓\n",
      "  [4/12] Oosaravelli                              [SAW] ✓\n",
      "  [5/12] Dammu                                    [SAW] ✓\n",
      "  [6/12] Janatha Garage                           [SAW] ✓\n",
      "  [7/12] Temper                                   [SAW] ✓\n",
      "  [8/12] Nannaku Prematho                         [SAW] ✓\n",
      "  [9/12] Jai Lava Kusa                            [SAW] ✓\n",
      "  [10/12] Aravinda Sametha                         [SAW] ✓\n",
      "  [11/12] RRR                                      [SAW] ✓\n",
      "  [12/12] Devara                                   [SAW] ✓\n",
      "\n",
      "================================================================================\n",
      "[2/6] HERO: Mahesh Babu (6 movies)\n",
      "================================================================================\n",
      "  [1/6] Bharat Ane Nenu                          [SAW] ✓\n",
      "  [2/6] Maharshi                                 [SAW] ✓\n",
      "  [3/6] Sarileru Neekevvaru                      [SAW] ✓\n",
      "  [4/6] Sarkaru Vaari Paata                      [SAW] ✓\n",
      "  [5/6] Guntur Kaaram                            [SAW] ✓\n",
      "  [6/6] Spyder                                   [SAW] ✓\n",
      "\n",
      "================================================================================\n",
      "[3/6] HERO: Pawan Kalyan (9 movies)\n",
      "================================================================================\n",
      "  [1/9] Katamarayudu                             [SAW] ✓\n",
      "  [2/9] Agnyaatavaasi                            [SAW] ✓\n",
      "  [3/9] Vakeel Saab                              [SAW] ✓\n",
      "  [4/9] Bheemla Nayak                            [SAW] ✓\n",
      "  [5/9] They Call Him OG                         [SAW] ✓\n",
      "  [6/9] Attarintiki Daredi                       [SAW] ✓\n",
      "  [7/9] Gabbar Singh                             [SAW] ✓\n",
      "  [8/9] Cameraman Gangatho Rambabu               [SAW] ✓\n",
      "  [9/9] Teen Maar                                [SAW] ✓\n",
      "\n",
      "================================================================================\n",
      "[4/6] HERO: Allu Arjun (6 movies)\n",
      "================================================================================\n",
      "  [1/6] Naa Peru Surya                           [SAW] ✓\n",
      "  [2/6] Ala Vaikunthapurramulo                   [SAW] ✓\n",
      "  [3/6] Pushpa: The Rise                         [SAW] ✓\n",
      "  [4/6] Pushpa 2: The Rule                       [SAW] ✓\n",
      "  [5/6] Parugu                                   [SAW] ✓\n",
      "  [6/6] Rudhramadevi                             [SAW] ✓\n",
      "\n",
      "================================================================================\n",
      "[5/6] HERO: Ram Charan (9 movies)\n",
      "================================================================================\n",
      "  [1/9] Govindudu Andarivadele                   [SAW] ✓\n",
      "  [2/9] Dhruva                                   [SAW] ✓\n",
      "  [3/9] Rangasthalam                             [SAW] ✓\n",
      "  [4/9] Vinaya Vidheya Rama                      [SAW] ✓\n",
      "  [5/9] Acharya                                  [SAW] ✓\n",
      "  [6/9] RRR                                      [SAW] ✓\n",
      "  [7/9] Bruce Lee                                [SAW] ✓\n",
      "  [8/9] Toofaan                                  [SAW] ✓\n",
      "  [9/9] Game Changer                             [SAW] ✓\n",
      "\n",
      "================================================================================\n",
      "[6/6] HERO: Prabhas (9 movies)\n",
      "================================================================================\n",
      "  [1/9] Baahubali: The Beginning                 [SAW] ✓\n",
      "  [2/9] Baahubali 2: The Conclusion              [SAW] ✓\n",
      "  [3/9] Saaho                                    [SAW] ✓\n",
      "  [4/9] Radhe Shyam                              [SAW] ✓\n",
      "  [5/9] Adipurush                                [SAW] ✓\n",
      "  [6/9] Salaar                                   [SAW] ✓\n",
      "  [7/9] Kalki 2898 AD                            [SAW] ✓\n",
      "  [8/9] Darling                                  [SAW] ✓\n",
      "  [9/9] Mr. Perfect                              [SAW] ✓\n",
      "\n",
      "================================================================================\n",
      "✓ Data saved to 'tfi_box_office_improved.csv'\n",
      "✓ Total movies processed: 51\n",
      "================================================================================\n",
      "\n",
      "Scraping complete! Check the CSV for results.\n",
      "Note: Success rate for Day 1 & Verdict is much higher now due to table parsing and better regex.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Top TFI Heroes with their complete filmography\n",
    "heroes_movies = {\n",
    "    'Jr NTR': [\n",
    "        # 'Student No: 1', 'Subbu', 'Ninnu Choodalani', 'Aadi', 'Santosham', 'Simhadri',\n",
    "        # 'Kadha Parthu', 'Shivamani', 'Andhrodu', 'Rakhi', 'Ashok', 'Yamadonga',\n",
    "        'Kantri', 'Adhurs', 'Brindavanam', 'Oosaravelli', 'Dammu', 'Janatha Garage',\n",
    "        'Temper', 'Nannaku Prematho', 'Jai Lava Kusa', 'Aravinda Sametha', 'RRR', 'Devara'\n",
    "    ],\n",
    "    'Mahesh Babu': [\n",
    "        # 'Rajakumarudu', 'Yuvarajuv', 'Vamsi', 'Okkadu', 'Nijam', 'Neeku Nenu Naku Nuvvu',\n",
    "        # 'Tagore', 'Athadu', 'Pokiri', 'Sainikudu', 'Dhookudu', 'Businessman', \n",
    "        # 'Seethamma Vakitlo Sirimalle Chettu', '1 Nenokkadine', 'Aagadu', 'Srimanthudu', \n",
    "        'Bharat Ane Nenu', 'Maharshi', 'Sarileru Neekevvaru', 'Sarkaru Vaari Paata', 'Guntur Kaaram', 'Spyder'\n",
    "    ],\n",
    "    'Pawan Kalyan': [\n",
    "        # 'Akkada Ammayi Ikkada Abbayi', 'Tholi Prema', 'Thammudu', 'Badri', 'Kushi', 'Johnny',\n",
    "        # 'Gudumba Shankar', 'Balu ABCDEFG', 'Bangaram', 'Annavaram', 'Shankar Dada MBBS', \n",
    "        'Katamarayudu', 'Agnyaatavaasi', 'Vakeel Saab', 'Bheemla Nayak', 'They Call Him OG', \n",
    "        'Attarintiki Daredi', 'Gabbar Singh', 'Cameraman Gangatho Rambabu', 'Teen Maar'\n",
    "    ],\n",
    "    'Allu Arjun': [\n",
    "        # 'Gangotri', 'Arya', 'Desamudu', 'Happy', 'Bunny', 'Arya 2', 'Vedam', 'Badrinath',\n",
    "        # 'Julai', 'Race Gurram', 'S/O Satyamurthy', 'Sarainodu', 'Duvvada Jagannadham', \n",
    "        'Naa Peru Surya', 'Ala Vaikunthapurramulo', 'Pushpa: The Rise', 'Pushpa 2: The Rule', 'Parugu', 'Rudhramadevi'\n",
    "    ],\n",
    "    'Ram Charan': [\n",
    "        # 'Chirutha', 'Magadheera', 'Orange', 'Leader', 'Racha', 'Naayak', 'Zanjeer', 'Yevadu',\n",
    "        'Govindudu Andarivadele', 'Dhruva', 'Rangasthalam', 'Vinaya Vidheya Rama', 'Acharya', 'RRR',\n",
    "        'Bruce Lee', 'Toofaan', 'Game Changer'\n",
    "    ],\n",
    "    'Prabhas': [\n",
    "        # 'Eeswar', 'Raghavendra', 'Varsham', 'Adavi Ramudu', 'Chatrapathi', 'Bujjigadu', 'Billa', 'Mirchi',\n",
    "        'Baahubali: The Beginning', 'Baahubali 2: The Conclusion', 'Saaho', 'Radhe Shyam', 'Adipurush',\n",
    "        'Salaar', 'Kalki 2898 AD', 'Darling', 'Mr. Perfect'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def extract_from_tables(soup):\n",
    "    \"\"\"Extract total WW, Day 1 WW, and Verdict from tables\"\"\"\n",
    "    data = {'total_WW_cls': '', 'day1_WW_Gross_cr': '', 'verdict': ''}\n",
    "    for table in soup.find_all('table'):\n",
    "        rows = table.find_all('tr')\n",
    "        for row in rows:\n",
    "            cells = row.find_all(['th', 'td'])\n",
    "            if len(cells) >= 2:\n",
    "                header = cells[0].get_text(strip=True).lower()\n",
    "                value = cells[1].get_text(strip=True)\n",
    "                \n",
    "                if 'worldwide' in header and ('gross' in header or 'collection' in header):\n",
    "                    match = re.search(r'₹?([\\d,]+\\.?\\d*)', value)\n",
    "                    if match:\n",
    "                        data['total_WW_cls'] = match.group(1)\n",
    "                \n",
    "                if ('day 1' in header or 'opening day' in header or 'first day' in header) and ('worldwide' in header or 'gross' in header):\n",
    "                    match = re.search(r'₹?([\\d,]+\\.?\\d*)', value)\n",
    "                    if match:\n",
    "                        data['day1_WW_Gross_cr'] = match.group(1)\n",
    "                \n",
    "                if 'verdict' in header:\n",
    "                    data['verdict'] = value.strip().title()\n",
    "    return data\n",
    "\n",
    "def extract_from_text(page_text):\n",
    "    \"\"\"Fallback extraction using improved regex\"\"\"\n",
    "    data = {'total_WW_cls': '', 'day1_WW_Gross_cr': '', 'verdict': ''}\n",
    "    \n",
    "    # Verdict first (explicit)\n",
    "    verdict_match = re.search(r'Verdict\\s*[:\\-]?\\s*([A-Za-z\\s\\-]+)', page_text, re.IGNORECASE)\n",
    "    if verdict_match:\n",
    "        v = verdict_match.group(1).strip().title()\n",
    "        if v in ['Hit', 'Super Hit', 'Blockbuster', 'Flop', 'Average', 'Disaster', 'Superhit']:\n",
    "            data['verdict'] = v.replace('Super Hit', 'Superhit')\n",
    "    \n",
    "    # Total WW\n",
    "    total_patterns = [\n",
    "        r'Worldwide.*?Gross.*?₹?([\\d,]+\\.?\\d*)\\s*(Cr|Crore)?',\n",
    "        r'Total.*?Collection.*?₹?([\\d,]+\\.?\\d*)\\s*(Cr|Crore)?',\n",
    "        r'Final.*?Total.*?₹?([\\d,]+\\.?\\d*)\\s*(Cr|Crore)?',\n",
    "        r'Worldwide.*?Collection.*?₹?([\\d,]+\\.?\\d*)\\s*(Cr|Crore)?',\n",
    "    ]\n",
    "    for pat in total_patterns:\n",
    "        m = re.search(pat, page_text, re.IGNORECASE)\n",
    "        if m and not data['total_WW_cls']:\n",
    "            data['total_WW_cls'] = m.group(1)\n",
    "    \n",
    "    # Day 1 WW\n",
    "    day1_patterns = [\n",
    "        r'Day\\s*1.*?Worldwide.*?₹?([\\d,]+\\.?\\d*)\\s*(Cr|Crore)?',\n",
    "        r'Opening\\s*Day.*?Gross.*?₹?([\\d,]+\\.?\\d*)\\s*(Cr|Crore)?',\n",
    "        r'First\\s*Day.*?Worldwide.*?₹?([\\d,]+\\.?\\d*)\\s*(Cr|Crore)?',\n",
    "        r'Day\\s*1.*?Gross.*?₹?([\\d,]+\\.?\\d*)\\s*(Cr|Crore)?',\n",
    "    ]\n",
    "    for pat in day1_patterns:\n",
    "        m = re.search(pat, page_text, re.IGNORECASE)\n",
    "        if m and not data['day1_WW_Gross_cr']:\n",
    "            data['day1_WW_Gross_cr'] = m.group(1)\n",
    "    \n",
    "    # Fallback keyword verdict\n",
    "    if not data['verdict']:\n",
    "        lower = page_text.lower()\n",
    "        if 'blockbuster' in lower:\n",
    "            data['verdict'] = 'Blockbuster'\n",
    "        elif 'superhit' in lower or 'super hit' in lower:\n",
    "            data['verdict'] = 'Superhit'\n",
    "        elif 'hit' in lower and 'flop' not in lower:\n",
    "            data['verdict'] = 'Hit'\n",
    "        elif 'flop' in lower or 'disaster' in lower:\n",
    "            data['verdict'] = 'Flop'\n",
    "        elif 'average' in lower:\n",
    "            data['verdict'] = 'Average'\n",
    "    \n",
    "    return data\n",
    "\n",
    "def scrape_sacnilk(movie_name):\n",
    "    try:\n",
    "        search_url = f\"https://www.sacnilk.com/box-office/?s={movie_name.replace(' ', '+')}\"\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find movie link (improved selector)\n",
    "        link = soup.find('a', href=re.compile(r'/news/.*_Box_Office_Collection'))\n",
    "        if not link:\n",
    "            return {}\n",
    "        \n",
    "        movie_url = link['href']\n",
    "        if not movie_url.startswith('http'):\n",
    "            movie_url = 'https://www.sacnilk.com' + movie_url\n",
    "        \n",
    "        resp = requests.get(movie_url, headers=headers, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "        movie_soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "        \n",
    "        # First try tables\n",
    "        data = extract_from_tables(movie_soup)\n",
    "        if data['total_WW_cls'] or data['day1_WW_Gross_cr'] or data['verdict']:\n",
    "            return data\n",
    "        \n",
    "        # Fallback to full text\n",
    "        page_text = movie_soup.get_text(separator=' ')\n",
    "        return extract_from_text(page_text)\n",
    "    \n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def scrape_andhra_box_office(movie_name):\n",
    "    try:\n",
    "        search_url = f\"https://www.andhraboxoffice.com/?s={movie_name.replace(' ', '+')}\"\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        link = soup.find('a', href=re.compile(r'/info.aspx'))\n",
    "        if not link:\n",
    "            return {}\n",
    "        \n",
    "        movie_url = link['href']\n",
    "        if not movie_url.startswith('http'):\n",
    "            movie_url = 'https://www.andhraboxoffice.com' + movie_url\n",
    "        \n",
    "        resp = requests.get(movie_url, headers=headers, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "        movie_soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "        \n",
    "        data = extract_from_tables(movie_soup)\n",
    "        if data['total_WW_cls'] or data['day1_WW_Gross_cr'] or data['verdict']:\n",
    "            return data\n",
    "        \n",
    "        page_text = movie_soup.get_text(separator=' ')\n",
    "        return extract_from_text(page_text)\n",
    "    \n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def scrape_wikipedia(movie_name):\n",
    "    try:\n",
    "        url = f\"https://en.wikipedia.org/wiki/{movie_name.replace(' ', '_')}\"\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        infobox = soup.find('table', class_='infobox')\n",
    "        \n",
    "        info = {'hero_name': '', 'year_of_release': ''}\n",
    "        if infobox:\n",
    "            for row in infobox.find_all('tr'):\n",
    "                header = row.find('th')\n",
    "                data_cell = row.find('td')\n",
    "                if header and data_cell:\n",
    "                    h_text = header.get_text(strip=True).lower()\n",
    "                    d_text = data_cell.get_text(strip=True)\n",
    "                    if 'starring' in h_text or 'cast' in h_text:\n",
    "                        links = data_cell.find_all('a')\n",
    "                        if links:\n",
    "                            info['hero_name'] = links[0].get_text(strip=True)\n",
    "                    if 'release date' in h_text or 'released' in h_text:\n",
    "                        year = re.search(r'\\b(19|20)\\d{2}\\b', d_text)\n",
    "                        if year:\n",
    "                            info['year_of_release'] = year.group(0)\n",
    "        return info\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def scrape_all_heroes():\n",
    "    all_movies_data = []\n",
    "    total_heroes = len(heroes_movies)\n",
    "    current_hero = 0\n",
    "    \n",
    "    for hero, movies in heroes_movies.items():\n",
    "        current_hero += 1\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"[{current_hero}/{total_heroes}] HERO: {hero} ({len(movies)} movies)\")\n",
    "        print('='*80)\n",
    "        \n",
    "        for idx, movie_name in enumerate(movies, 1):\n",
    "            print(f\"  [{idx}/{len(movies)}] {movie_name:40}\", end=' ')\n",
    "            \n",
    "            print(\"[S\", end='', flush=True)\n",
    "            sac_data = scrape_sacnilk(movie_name)\n",
    "            \n",
    "            print(\"A\", end='', flush=True)\n",
    "            abo_data = scrape_andhra_box_office(movie_name)\n",
    "            \n",
    "            print(\"W\", end='', flush=True)\n",
    "            wiki_data = scrape_wikipedia(movie_name)\n",
    "            print(\"]\", end=' ')\n",
    "            \n",
    "            movie_info = {\n",
    "                'movie_name': movie_name,\n",
    "                'hero_name': wiki_data.get('hero_name') or hero,\n",
    "                'total_WW_cls': sac_data.get('total_WW_cls') or abo_data.get('total_WW_cls', ''),\n",
    "                'day1_WW_Gross_cr': sac_data.get('day1_WW_Gross_cr') or abo_data.get('day1_WW_Gross_cr', ''),\n",
    "                'verdict': sac_data.get('verdict') or abo_data.get('verdict', ''),\n",
    "                'year_of_release': wiki_data.get('year_of_release', '')\n",
    "            }\n",
    "            \n",
    "            all_movies_data.append(movie_info)\n",
    "            print(\"✓\")\n",
    "            time.sleep(1)  # Increased delay to be respectful\n",
    "    \n",
    "    return all_movies_data\n",
    "\n",
    "def save_to_csv(data, filename='tfi_box_office_improved.csv'):\n",
    "    if not data:\n",
    "        print(\"\\nNo data to save!\")\n",
    "        return\n",
    "    \n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['movie_name', 'hero_name', 'total_WW_cls', 'day1_WW_Gross_cr', 'verdict', 'year_of_release']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"✓ Data saved to '{filename}'\")\n",
    "    print(f\"✓ Total movies processed: {len(data)}\")\n",
    "    print('='*80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TFI HEROES BOX OFFICE DATA SCRAPER (Improved 2025 Version)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Sources: Sacnilk (primary), AndhraBoxOffice, Wikipedia\")\n",
    "    \n",
    "    movie_data = scrape_all_heroes()\n",
    "    save_to_csv(movie_data)\n",
    "    \n",
    "    print(\"\\nScraping complete! Check the CSV for results.\")\n",
    "    print(\"Note: Success rate for Day 1 & Verdict is much higher now due to table parsing and better regex.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dfdf80",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "358ffa7c",
   "metadata": {},
   "source": [
    "GROK2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d654569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TFI HEROES BOX OFFICE DATA SCRAPER (Working 2025 Version)\n",
      "================================================================================\n",
      "Sources: Sacnilk, TrackTollywood, AndhraBoxOffice, Wikipedia\n",
      "Improved selectors and regex for better extraction.\n",
      "\n",
      "================================================================================\n",
      "[1/6] HERO: Jr NTR (12 movies)\n",
      "================================================================================\n",
      "  [1/12] Kantri                                        SAndhra error for Kantri: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [2/12] Adhurs                                        SAndhra error for Adhurs: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [3/12] Brindavanam                                   SAndhra error for Brindavanam: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [4/12] Oosaravelli                                   SAndhra error for Oosaravelli: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [5/12] Dammu                                         SAndhra error for Dammu: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [6/12] Janatha Garage                                SAndhra error for Janatha Garage: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [7/12] Temper                                        SAndhra error for Temper: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [8/12] Nannaku Prematho                              SAndhra error for Nannaku Prematho: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [9/12] Jai Lava Kusa                                 SAndhra error for Jai Lava Kusa: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [10/12] Aravinda Sametha                              SAndhra error for Aravinda Sametha: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [11/12] RRR                                           SAndhra error for RRR: HTTPSConnectionPool(host='www.andhraboxoffice.com', port=443): Read timed out. (read timeout=15)\n",
      "ATW ✓\n",
      "  [12/12] Devara                                        SAndhra error for Devara: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "\n",
      "================================================================================\n",
      "[2/6] HERO: Mahesh Babu (6 movies)\n",
      "================================================================================\n",
      "  [1/6] Bharat Ane Nenu                               SAndhra error for Bharat Ane Nenu: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [2/6] Maharshi                                      SAndhra error for Maharshi: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [3/6] Sarileru Neekevvaru                           SAndhra error for Sarileru Neekevvaru: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [4/6] Sarkaru Vaari Paata                           SAndhra error for Sarkaru Vaari Paata: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [5/6] Guntur Kaaram                                 SAndhra error for Guntur Kaaram: HTTPSConnectionPool(host='www.andhraboxoffice.com', port=443): Read timed out. (read timeout=15)\n",
      "ATW ✓\n",
      "  [6/6] Spyder                                        SAndhra error for Spyder: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "\n",
      "================================================================================\n",
      "[3/6] HERO: Pawan Kalyan (9 movies)\n",
      "================================================================================\n",
      "  [1/9] Katamarayudu                                  SAndhra error for Katamarayudu: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [2/9] Agnyaatavaasi                                 SAndhra error for Agnyaatavaasi: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATWikipedia error for Agnyaatavaasi: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Agnyaatavaasi\n",
      "W ✓\n",
      "  [3/9] Vakeel Saab                                   SAndhra error for Vakeel Saab: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [4/9] Bheemla Nayak                                 SAndhra error for Bheemla Nayak: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [5/9] They Call Him OG                              SAndhra error for They Call Him OG: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [6/9] Attarintiki Daredi                            SAndhra error for Attarintiki Daredi: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [7/9] Gabbar Singh                                  SAndhra error for Gabbar Singh: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [8/9] Cameraman Gangatho Rambabu                    SAndhra error for Cameraman Gangatho Rambabu: HTTPSConnectionPool(host='www.andhraboxoffice.com', port=443): Read timed out. (read timeout=15)\n",
      "ATW ✓\n",
      "  [9/9] Teen Maar                                     SAndhra error for Teen Maar: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "\n",
      "================================================================================\n",
      "[4/6] HERO: Allu Arjun (6 movies)\n",
      "================================================================================\n",
      "  [1/6] Naa Peru Surya                                SAndhra error for Naa Peru Surya: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [2/6] Ala Vaikunthapurramulo                        Sacnilk error for Ala Vaikunthapurramulo: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "SAndhra error for Ala Vaikunthapurramulo: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATWikipedia error for Ala Vaikunthapurramulo: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Ala_Vaikunthapurramulo\n",
      "W ✓\n",
      "  [3/6] Pushpa: The Rise                              SAndhra error for Pushpa: The Rise: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [4/6] Pushpa 2: The Rule                            SAndhra error for Pushpa 2: The Rule: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [5/6] Parugu                                        SAndhra error for Parugu: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [6/6] Rudhramadevi                                  SAndhra error for Rudhramadevi: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "\n",
      "================================================================================\n",
      "[5/6] HERO: Ram Charan (9 movies)\n",
      "================================================================================\n",
      "  [1/9] Govindudu Andarivadele                        SAndhra error for Govindudu Andarivadele: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [2/9] Dhruva                                        SAndhra error for Dhruva: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [3/9] Rangasthalam                                  SAndhra error for Rangasthalam: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [4/9] Vinaya Vidheya Rama                           SAndhra error for Vinaya Vidheya Rama: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [5/9] Acharya                                       SAndhra error for Acharya: HTTPSConnectionPool(host='www.andhraboxoffice.com', port=443): Read timed out. (read timeout=15)\n",
      "ATW ✓\n",
      "  [6/9] RRR                                           SAndhra error for RRR: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [7/9] Bruce Lee                                     SAndhra error for Bruce Lee: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [8/9] Toofaan                                       SAndhra error for Toofaan: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [9/9] Game Changer                                  SAndhra error for Game Changer: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "\n",
      "================================================================================\n",
      "[6/6] HERO: Prabhas (9 movies)\n",
      "================================================================================\n",
      "  [1/9] Baahubali: The Beginning                      SAndhra error for Baahubali: The Beginning: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [2/9] Baahubali 2: The Conclusion                   SAndhra error for Baahubali 2: The Conclusion: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [3/9] Saaho                                         SAndhra error for Saaho: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [4/9] Radhe Shyam                                   SAndhra error for Radhe Shyam: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [5/9] Adipurush                                     SAndhra error for Adipurush: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [6/9] Salaar                                        SAndhra error for Salaar: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [7/9] Kalki 2898 AD                                 SAndhra error for Kalki 2898 AD: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [8/9] Darling                                       SAndhra error for Darling: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "  [9/9] Mr. Perfect                                   SAndhra error for Mr. Perfect: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "ATW ✓\n",
      "\n",
      "================================================================================\n",
      "✓ Data saved to 'tfi_box_office_working.csv'\n",
      "✓ Total movies processed: 51\n",
      "================================================================================\n",
      "\n",
      "Scraping complete! Check the CSV - should have more populated fields now.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Top TFI Heroes with their filmography (focused on recent major films for better data availability)\n",
    "heroes_movies = {\n",
    "    'Jr NTR': [\n",
    "        'Kantri', 'Adhurs', 'Brindavanam', 'Oosaravelli', 'Dammu', 'Janatha Garage',\n",
    "        'Temper', 'Nannaku Prematho', 'Jai Lava Kusa', 'Aravinda Sametha', 'RRR', 'Devara'\n",
    "    ],\n",
    "    'Mahesh Babu': [\n",
    "        'Bharat Ane Nenu', 'Maharshi', 'Sarileru Neekevvaru', 'Sarkaru Vaari Paata', 'Guntur Kaaram', 'Spyder'\n",
    "    ],\n",
    "    'Pawan Kalyan': [\n",
    "        'Katamarayudu', 'Agnyaatavaasi', 'Vakeel Saab', 'Bheemla Nayak', 'They Call Him OG', \n",
    "        'Attarintiki Daredi', 'Gabbar Singh', 'Cameraman Gangatho Rambabu', 'Teen Maar'\n",
    "    ],\n",
    "    'Allu Arjun': [\n",
    "        'Naa Peru Surya', 'Ala Vaikunthapurramulo', 'Pushpa: The Rise', 'Pushpa 2: The Rule', 'Parugu', 'Rudhramadevi'\n",
    "    ],\n",
    "    'Ram Charan': [\n",
    "        'Govindudu Andarivadele', 'Dhruva', 'Rangasthalam', 'Vinaya Vidheya Rama', 'Acharya', 'RRR',\n",
    "        'Bruce Lee', 'Toofaan', 'Game Changer'\n",
    "    ],\n",
    "    'Prabhas': [\n",
    "        'Baahubali: The Beginning', 'Baahubali 2: The Conclusion', 'Saaho', 'Radhe Shyam', 'Adipurush',\n",
    "        'Salaar', 'Kalki 2898 AD', 'Darling', 'Mr. Perfect'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def extract_from_tables(soup):\n",
    "    data = {'total_WW_cls': '', 'day1_WW_Gross_cr': '', 'verdict': ''}\n",
    "    for table in soup.find_all('table'):\n",
    "        rows = table.find_all('tr')\n",
    "        for row in rows:\n",
    "            cells = row.find_all(['th', 'td'])\n",
    "            if len(cells) >= 2:\n",
    "                header = cells[0].get_text(strip=True).lower()\n",
    "                value = ' '.join(cells[1].stripped_strings)\n",
    "                \n",
    "                # Total WW\n",
    "                if 'worldwide' in header and any(word in header for word in ['gross', 'collection', 'total']):\n",
    "                    match = re.search(r'[\\d,]+(?:\\.\\d+)?', value.replace('₹', ''))\n",
    "                    if match:\n",
    "                        data['total_WW_cls'] = match.group(0)\n",
    "                \n",
    "                # Day 1\n",
    "                if any(day_term in header for day_term in ['day 1', 'opening day', 'first day']) and any(gross_term in header for gross_term in ['worldwide', 'gross']):\n",
    "                    match = re.search(r'[\\d,]+(?:\\.\\d+)?', value.replace('₹', ''))\n",
    "                    if match:\n",
    "                        data['day1_WW_Gross_cr'] = match.group(0)\n",
    "                \n",
    "                # Verdict\n",
    "                if 'verdict' in header:\n",
    "                    data['verdict'] = value.strip().title()\n",
    "    return data\n",
    "\n",
    "def extract_from_text(page_text):\n",
    "    data = {'total_WW_cls': '', 'day1_WW_Gross_cr': '', 'verdict': ''}\n",
    "    \n",
    "    # Verdict explicit\n",
    "    verdict_match = re.search(r'Verdict\\s*[:\\-]?\\s*([A-Za-z\\s\\-]+)', page_text, re.IGNORECASE)\n",
    "    if verdict_match:\n",
    "        v = verdict_match.group(1).strip().title()\n",
    "        valid_verdicts = ['Hit', 'Super Hit', 'Blockbuster', 'Flop', 'Average', 'Disaster', 'Superhit', 'All Time Blockbuster']\n",
    "        if any(vv in v for vv in valid_verdicts):\n",
    "            data['verdict'] = v.replace('Super Hit', 'Superhit').replace('All Time', 'All-Time')\n",
    "    \n",
    "    # Total WW patterns (more flexible)\n",
    "    total_patterns = [\n",
    "        r'Worldwide\\s*(?:Gross|Collection|Total)[:\\s]*[\\d,]+(?:\\.\\d+)?\\s*(?:Cr|Crore)',\n",
    "        r'Total\\s*Worldwide\\s*[\\d,]+(?:\\.\\d+)?\\s*(?:Cr|Crore)',\n",
    "        r'Final\\s*(?:Gross|Collection)\\s*[\\d,]+(?:\\.\\d+)?\\s*(?:Cr|Crore)',\n",
    "    ]\n",
    "    for pat in total_patterns:\n",
    "        m = re.search(pat, page_text, re.IGNORECASE)\n",
    "        if m:\n",
    "            num_match = re.search(r'[\\d,]+(?:\\.\\d+)?', m.group(0))\n",
    "            if num_match and not data['total_WW_cls']:\n",
    "                data['total_WW_cls'] = num_match.group(0)\n",
    "                break\n",
    "    \n",
    "    # Day 1 patterns\n",
    "    day1_patterns = [\n",
    "        r'Day\\s*1\\s*(?:Worldwide\\s*)?(?:Gross|Collection)[:\\s]*[\\d,]+(?:\\.\\d+)?\\s*(?:Cr|Crore)',\n",
    "        r'Opening\\s*Day\\s*(?:Worldwide\\s*)?[\\d,]+(?:\\.\\d+)?\\s*(?:Cr|Crore)',\n",
    "        r'First\\s*Day\\s*(?:Gross|Worldwide)[\\d,]+(?:\\.\\d+)?\\s*(?:Cr|Crore)',\n",
    "    ]\n",
    "    for pat in day1_patterns:\n",
    "        m = re.search(pat, page_text, re.IGNORECASE)\n",
    "        if m:\n",
    "            num_match = re.search(r'[\\d,]+(?:\\.\\d+)?', m.group(0))\n",
    "            if num_match and not data['day1_WW_Gross_cr']:\n",
    "                data['day1_WW_Gross_cr'] = num_match.group(0)\n",
    "                break\n",
    "    \n",
    "    # Fallback verdict\n",
    "    if not data['verdict']:\n",
    "        lower_text = page_text.lower()\n",
    "        if 'all time blockbuster' in lower_text:\n",
    "            data['verdict'] = 'All-Time Blockbuster'\n",
    "        elif 'blockbuster' in lower_text:\n",
    "            data['verdict'] = 'Blockbuster'\n",
    "        elif 'superhit' in lower_text or 'super hit' in lower_text:\n",
    "            data['verdict'] = 'Superhit'\n",
    "        elif 'hit' in lower_text and not any(f in lower_text for f in ['flop', 'disaster']):\n",
    "            data['verdict'] = 'Hit'\n",
    "        elif any(f in lower_text for f in ['flop', 'disaster']):\n",
    "            data['verdict'] = 'Flop'\n",
    "        elif 'average' in lower_text:\n",
    "            data['verdict'] = 'Average'\n",
    "    \n",
    "    return data\n",
    "\n",
    "def scrape_sacnilk(movie_name):\n",
    "    try:\n",
    "        # Use box-office search for better results\n",
    "        search_url = f\"https://www.sacnilk.com/box-office/?s={movie_name.replace(' ', '+')}\"\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find link to box office page\n",
    "        link = soup.find('a', href=re.compile(r'/news/.*Box.*Office', re.I))\n",
    "        if not link:\n",
    "            # Fallback to movie page\n",
    "            link = soup.find('a', string=re.compile(movie_name, re.I))\n",
    "        if not link:\n",
    "            return {}\n",
    "        \n",
    "        movie_url = link['href']\n",
    "        if not movie_url.startswith('http'):\n",
    "            movie_url = 'https://www.sacnilk.com' + movie_url\n",
    "        \n",
    "        resp = requests.get(movie_url, headers=headers, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "        movie_soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "        \n",
    "        data = extract_from_tables(movie_soup)\n",
    "        if any(data.values()):\n",
    "            return data\n",
    "        \n",
    "        page_text = movie_soup.get_text(separator=' ')\n",
    "        return extract_from_text(page_text)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Sacnilk error for {movie_name}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def scrape_andhra_box_office(movie_name):\n",
    "    try:\n",
    "        search_url = f\"https://www.andhraboxoffice.com/?s={movie_name.replace(' ', '+')}\"\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find info page\n",
    "        link = soup.find('a', href=re.compile(r'/info\\.aspx'))\n",
    "        if not link:\n",
    "            return {}\n",
    "        \n",
    "        movie_url = link['href']\n",
    "        if not movie_url.startswith('http'):\n",
    "            movie_url = 'https://www.andhraboxoffice.com' + movie_url\n",
    "        \n",
    "        resp = requests.get(movie_url, headers=headers, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "        movie_soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "        \n",
    "        data = extract_from_tables(movie_soup)\n",
    "        if any(data.values()):\n",
    "            return data\n",
    "        \n",
    "        page_text = movie_soup.get_text(separator=' ')\n",
    "        return extract_from_text(page_text)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Andhra error for {movie_name}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def scrape_tracktollywood(movie_name):\n",
    "    try:\n",
    "        search_url = f\"https://tracktollywood.com/?s={movie_name.replace(' ', '+')}\"\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find box office link\n",
    "        link = soup.find('a', href=re.compile(r'box-office', re.I))\n",
    "        if not link:\n",
    "            # Fallback to first relevant link\n",
    "            link = soup.find('a', string=re.compile(movie_name, re.I))\n",
    "        if not link:\n",
    "            return {}\n",
    "        \n",
    "        movie_url = link['href']\n",
    "        if not movie_url.startswith('http'):\n",
    "            movie_url = 'https://tracktollywood.com' + movie_url\n",
    "        \n",
    "        resp = requests.get(movie_url, headers=headers, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "        movie_soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "        \n",
    "        data = extract_from_tables(movie_soup)\n",
    "        if any(data.values()):\n",
    "            return data\n",
    "        \n",
    "        page_text = movie_soup.get_text(separator=' ')\n",
    "        return extract_from_text(page_text)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"TrackTollywood error for {movie_name}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def scrape_wikipedia(movie_name):\n",
    "    try:\n",
    "        # Clean movie name for Wikipedia\n",
    "        wiki_name = movie_name.replace(':', '').replace(' ', '_').replace('2:', '2')\n",
    "        url = f\"https://en.wikipedia.org/wiki/{wiki_name}\"\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        infobox = soup.find('table', class_='infobox')\n",
    "        \n",
    "        info = {'hero_name': '', 'year_of_release': ''}\n",
    "        if infobox:\n",
    "            for row in infobox.find_all('tr'):\n",
    "                header = row.find('th')\n",
    "                data_cell = row.find('td')\n",
    "                if header and data_cell:\n",
    "                    h_text = header.get_text(strip=True).lower()\n",
    "                    d_text = data_cell.get_text(strip=True)\n",
    "                    # Hero\n",
    "                    if 'starring' in h_text or 'cast' in h_text:\n",
    "                        links = data_cell.find_all('a')\n",
    "                        if links:\n",
    "                            info['hero_name'] = links[0].get_text(strip=True)\n",
    "                    # Year\n",
    "                    if 'release date' in h_text or 'released' in h_text:\n",
    "                        year = re.search(r'\\b(19|20)\\d{2}\\b', d_text)\n",
    "                        if year:\n",
    "                            info['year_of_release'] = year.group(0)\n",
    "        return info\n",
    "    except Exception as e:\n",
    "        print(f\"Wikipedia error for {movie_name}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def scrape_all_heroes():\n",
    "    all_movies_data = []\n",
    "    total_heroes = len(heroes_movies)\n",
    "    current_hero = 0\n",
    "    \n",
    "    for hero, movies in heroes_movies.items():\n",
    "        current_hero += 1\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"[{current_hero}/{total_heroes}] HERO: {hero} ({len(movies)} movies)\")\n",
    "        print('='*80)\n",
    "        \n",
    "        for idx, movie_name in enumerate(movies, 1):\n",
    "            print(f\"  [{idx}/{len(movies)}] {movie_name:45}\", end=' ')\n",
    "            \n",
    "            sac_data = scrape_sacnilk(movie_name)\n",
    "            print(\"S\", end='', flush=True)\n",
    "            \n",
    "            abo_data = scrape_andhra_box_office(movie_name)\n",
    "            print(\"A\", end='', flush=True)\n",
    "            \n",
    "            track_data = scrape_tracktollywood(movie_name)\n",
    "            print(\"T\", end='', flush=True)\n",
    "            \n",
    "            wiki_data = scrape_wikipedia(movie_name)\n",
    "            print(\"W\", end='', flush=True)\n",
    "            \n",
    "            # Merge with priority: Sacnilk > Track > Andhra\n",
    "            movie_info = {\n",
    "                'movie_name': movie_name,\n",
    "                'hero_name': wiki_data.get('hero_name', hero),\n",
    "                'total_WW_cls': sac_data.get('total_WW_cls') or track_data.get('total_WW_cls') or abo_data.get('total_WW_cls', ''),\n",
    "                'day1_WW_Gross_cr': sac_data.get('day1_WW_Gross_cr') or track_data.get('day1_WW_Gross_cr') or abo_data.get('day1_WW_Gross_cr', ''),\n",
    "                'verdict': sac_data.get('verdict') or track_data.get('verdict') or abo_data.get('verdict', ''),\n",
    "                'year_of_release': wiki_data.get('year_of_release', '')\n",
    "            }\n",
    "            \n",
    "            all_movies_data.append(movie_info)\n",
    "            print(\" ✓\")\n",
    "            time.sleep(2)  # Respectful delay\n",
    "    \n",
    "    return all_movies_data\n",
    "\n",
    "def save_to_csv(data, filename='tfi_box_office_working.csv'):\n",
    "    if not data:\n",
    "        print(\"\\nNo data to save!\")\n",
    "        return\n",
    "    \n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['movie_name', 'hero_name', 'total_WW_cls', 'day1_WW_Gross_cr', 'verdict', 'year_of_release']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"✓ Data saved to '{filename}'\")\n",
    "    print(f\"✓ Total movies processed: {len(data)}\")\n",
    "    print('='*80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TFI HEROES BOX OFFICE DATA SCRAPER (Working 2025 Version)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Sources: Sacnilk, TrackTollywood, AndhraBoxOffice, Wikipedia\")\n",
    "    print(\"Improved selectors and regex for better extraction.\")\n",
    "    \n",
    "    movie_data = scrape_all_heroes()\n",
    "    save_to_csv(movie_data)\n",
    "    \n",
    "    print(\"\\nScraping complete! Check the CSV - should have more populated fields now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6760a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
